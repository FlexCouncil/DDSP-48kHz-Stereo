{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddsp_train_and_timbre_transfer_48kHz_stereo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NpJd3dlOCStH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DemonFlexCouncil/DDSP-48kHz-Stereo/blob/master/ddsp/colab/ddsp_train_and_timbre_transfer_48kHz_stereo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hMqWDc_m6rUC"
      },
      "source": [
        "\n",
        "##### Copyright 2020 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VNhgka4UKNjf",
        "colab": {}
      },
      "source": [
        "# Copyright 2020 Google LLC. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SpXo6phTiOQM"
      },
      "source": [
        "# Train & Timbre Transfer--DDSP Autoencoder on GPU--48kHz/Stereo\n",
        "\n",
        "Made by [Google Magenta](https://magenta.tensorflow.org/)--altered by [Demon Flex Council](https://demonflexcouncil.wixsite.com/demonflexcouncil)\n",
        "\n",
        "This notebook demonstrates how to install the DDSP library and train it for synthesis based on your own data using command-line scripts. If run inside of Colaboratory, it will automatically use a free or Pro Google Cloud GPU, depending on your membership level.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/ddsp/additive_diagram/ddsp_autoencoder.png\" alt=\"DDSP Autoencoder figure\" width=\"700\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wXjcauVRB48S"
      },
      "source": [
        "**Note that bash commands are prefixed with a `!` inside of Colaboratory, but you would leave them out if running directly in a terminal.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ6V74mfv8rG",
        "colab_type": "text"
      },
      "source": [
        "**A Little Background**\n",
        "\n",
        "A producer friend of mine turned me on to Magenta’s DDSP, and I’m glad he did. In my mind it represents the way forward for AI music. Finally we have a glimpse inside the black box, with access to musical parameters as well as neural net hyperparameters. And DDSP leverages decades of studio knowledge by utilizing traditional processors like synthesizers and effects. One can envision a time when DDSP-like elements will sit at the heart of production DAWs.\n",
        "\n",
        "According to Magenta’s paper, this algorithm was intended as proof of concept, but I wanted to bend it more towards a tool for producers. I bumped the sample rate up to 48kHz and made it stereo. I also introduced a variable render length so you can feed it loops or phrases. However, there are limits to this parameter. The total number of samples in your render length (number of seconds * 48000) must be evenly divisible by 800. In practice, this means using round-numbered or highly-divisible tempos (105, 96, 90, 72, 50…) or using material that does not depend on tempo.\n",
        "\n",
        "Also note that longer render times may require a smaller batch size, which is currently set at 8 for a 4-second render. This may diminish audio quality, so use shorter render times if at all possible.\n",
        "\n",
        "You can train with or without latent vectors, z(t), for the audio. There is a tradeoff here. No latent vectors allows for more pronounced shifts in the “Modify Conditioning” section, but the rendered audio sounds cloudier. Then again, sometimes cloudier is better. The default mode is latent vectors.\n",
        "\n",
        "The dataset and audio primer files must be WAVE format, stereo, and 48kHz. Most DAWs and audio editors have a 48kHz export option, including the free [Audacity](https://www.audacityteam.org/). There appears to be a lower limit on the total size of the dataset, somewhere around 20MB. Anything lower than that and the TFRecord maker will create blank records (0 bytes). Also, Colaboratory may throw memory errors if it encounters large single audio files—cut the file into smaller pieces if this happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vn7CQ4GQizHy"
      },
      "source": [
        "## **Step 1**--Install Dependencies\n",
        "First we install the required dependencies with `pip` (takes about 5 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "VxPuPR0j5Gs7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "748938bd-9379-435a-bcf0-a8310e7ab24e"
      },
      "source": [
        "!pip install tensorflow==2.2\n",
        "!pip install mir_eval\n",
        "!pip install apache_beam\n",
        "!pip install crepe\n",
        "!pip install pydub\n",
        "!pip3 install ffmpeg-normalize\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiZnAIiQqP2s",
        "colab_type": "text"
      },
      "source": [
        "## **Step 2**--Confirm you are running Tensorflow version 2.2.0.\n",
        "\n",
        "This is the only version which will work with this notebook. If you see any other version than 2.2.0 below, factory restart your runtime (in the \"Runtime\" menu) and run Step 1 again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEvLVR8trGiH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "7441c180-25a1-4568-84b2-0c2afff1b541"
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L6MXUbL6KeMn"
      },
      "source": [
        "## **Step 3**--Login and mount your Google Drive\n",
        "\n",
        "This will require an authentication code. You should then be able to see your Drive in the file browser on the left panel--make sure you've clicked the folder icon on the far left side of your Internet browser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m33xuTjEKazJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "de9e5edf-b494-43fd-de01-3881ce3cf8ba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT9ASvNq4999",
        "colab_type": "text"
      },
      "source": [
        "## **Step 4**--Set render length\n",
        "\n",
        "Determines the length of audio slices for training and resynthesis. Decimals are OK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9p-35hx22sL",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "RENDER_SECONDS =  4.0#@param {type:\"number\", min:1, max:10}\n",
        "RENDER_SAMPLES = int(RENDER_SECONDS * 48000)\n",
        "\n",
        "if ((RENDER_SAMPLES % 800) != 0):\n",
        "  raise ValueError(\"Number of samples at 48kHz must be divisble by 800.\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECATHBbTvY3H",
        "colab_type": "text"
      },
      "source": [
        "## **Step 5**--Latent vectors mode\n",
        "\n",
        "Uncheck the box to train without z(t)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0KXh3D-vlxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LATENT_VECTORS = True #@param{type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bICWo4uclNCr",
        "colab_type": "text"
      },
      "source": [
        "## **Step 6**--Set your audio directory on Drive and get DDSP repository from Github\n",
        "\n",
        "Find a folder on Drive where you want to upload audio files and store checkpoints. Then right-click on the folder and select \"Copy path\". Enter the path below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BndBqU9mlo5i",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "056b3d75-6b66-424c-f0e4-a836d6d7dfbb"
      },
      "source": [
        "DRIVE_DIR =  \"/content/drive/My Drive/test\" #@param {type:\"string\"}\n",
        "\n",
        "if LATENT_VECTORS:\n",
        "  !git clone https://github.com/DemonFlexCouncil/DDSP-48kHz-Stereo.git\n",
        "else:\n",
        "  !git clone https://github.com/DemonFlexCouncil/DDSP-48kHz-Stereo-NoZ.git\n",
        "\n",
        "AUDIO_DIR = '/content/data/audio'\n",
        "!mkdir -p $AUDIO_DIR\n",
        "AUDIO_FILEPATTERN = AUDIO_DIR + '/*'\n",
        "AUDIO_INPUT_DIR = DRIVE_DIR + '/audio_input'\n",
        "AUDIO_OUTPUT_DIR = DRIVE_DIR + '/audio_output'\n",
        "CKPT_OUTPUT_DIR = DRIVE_DIR + '/ckpt'\n",
        "SAVE_DIR = os.path.join(DRIVE_DIR, 'model')\n",
        "\n",
        "%cd $DRIVE_DIR\n",
        "!mkdir -p audio_input audio_output ckpt data model primers"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a4vmxpj1LC7m"
      },
      "source": [
        "## **Step 7**--Upload your audio files to Drive and create a TFRecord dataset\n",
        "Put all of your training audio files in the \"audio_input\" directory inside the directory you set as DRIVE_DIR. The algorithm typically works well with audio from a single acoustic environment.\n",
        "\n",
        "Preprocessing involves inferring the fundamental frequency (or \"pitch\") with [CREPE](http://github.com/marl/crepe), and computing the loudness. These features will then be stored in a sharded [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) file for easier loading. Depending on the amount of input audio, this process usually takes a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARQyA8m0q9vb",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7dc5342a-f51b-46c2-8508-3305130aae15"
      },
      "source": [
        "audio_files = glob.glob(os.path.join(AUDIO_INPUT_DIR, '*.wav'))\n",
        "\n",
        "for fname in audio_files:\n",
        "  target_name = os.path.join(AUDIO_DIR, \n",
        "                             os.path.basename(fname).replace(' ', '_'))\n",
        "  print('Copying {} to {}'.format(fname, target_name))\n",
        "  !cp \"$fname\" $target_name\n",
        "\n",
        "TRAIN_TFRECORD = '/content/data/train.tfrecord'\n",
        "TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
        "\n",
        "drive_data_dir = os.path.join(DRIVE_DIR, 'data') \n",
        "drive_dataset_files = glob.glob(drive_data_dir + '/*')\n",
        "\n",
        "# Make a new dataset.\n",
        "if not glob.glob(AUDIO_FILEPATTERN):\n",
        "  raise ValueError('No audio files found. Please use the previous cell to '\n",
        "                    'upload.')\n",
        "\n",
        "if LATENT_VECTORS:  \n",
        "  !python /content/DDSP-48kHz-Stereo/ddsp/training/data_preparation/prepare_tfrecord.py \\\n",
        "    --input_audio_filepatterns=$AUDIO_FILEPATTERN \\\n",
        "    --output_tfrecord_path=$TRAIN_TFRECORD \\\n",
        "    --num_shards=10 \\\n",
        "    --example_secs=$RENDER_SECONDS \\\n",
        "    --alsologtostderr\n",
        "else:  \n",
        "  !python /content/DDSP-48kHz-Stereo-NoZ/ddsp/training/data_preparation/prepare_tfrecord.py \\\n",
        "    --input_audio_filepatterns=$AUDIO_FILEPATTERN \\\n",
        "    --output_tfrecord_path=$TRAIN_TFRECORD \\\n",
        "    --num_shards=10 \\\n",
        "    --example_secs=$RENDER_SECONDS \\\n",
        "    --alsologtostderr\n",
        "\n",
        "TRAIN_TFRECORD_DIR = DRIVE_DIR + '/data'\n",
        "TRAIN_TFRECORD_DIR = TRAIN_TFRECORD_DIR.replace(\"My Drive\", \"My\\ Drive\")\n",
        "!cp $TRAIN_TFRECORD_FILEPATTERN $TRAIN_TFRECORD_DIR"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d4toX-D-AYZL"
      },
      "source": [
        "## **Step 8**--Save dataset statistics for timbre transfer\n",
        "\n",
        "Quantile normalization helps match loudness of timbre transfer inputs to the \n",
        "loudness of the dataset, so let's calculate it here and save in a pickle file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bp_c8P0xApY6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6dfa7a30-f941-489c-8dfe-06b14d5ec362"
      },
      "source": [
        "if LATENT_VECTORS: \n",
        "  %cd /content/DDSP-48kHz-Stereo/ddsp/\n",
        "else:\n",
        "  %cd /content/DDSP-48kHz-Stereo-NoZ/ddsp/\n",
        "\n",
        "from colab import colab_utils\n",
        "from training import data\n",
        "\n",
        "TRAIN_TFRECORD = '/content/data/train.tfrecord'\n",
        "TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
        "\n",
        "data_provider = data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN, example_secs=RENDER_SECONDS)\n",
        "dataset = data_provider.get_dataset(shuffle=False)\n",
        "\n",
        "PICKLE_FILE_PATH = os.path.join(SAVE_DIR, 'dataset_statistics.pkl')\n",
        "\n",
        "colab_utils.save_dataset_statistics(data_provider, PICKLE_FILE_PATH)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoCFCynuaiK_",
        "colab_type": "text"
      },
      "source": [
        "## **Step 9**--Train model\n",
        "\n",
        "DDSP was designed to model a single instrument, but I've had more interesting results training it on sparse multi-timbral material. In this case, the neural network will attempt to model all timbres, but will likely associate certain timbres with different pitch and loudness conditions.\n",
        "\n",
        "Note that  [gin configuration](https://github.com/google/gin-config) files specify parameters for the both the model architecture (solo_instrument.gin) and the dataset (tfrecord.gin). These parameters can be overriden in the run script below (!python ddsp/ddsp_run.py).\n",
        "\n",
        "### Training Notes:\n",
        "* Models typically perform well when the loss drops to the range of ~7.0-8.5.\n",
        "* Depending on the dataset this can take anywhere from 30k-90k training steps usually.\n",
        "* The default is set to 90k, but you can stop training at any time (select \"Interrupt execution\" from the \"Runtime\" menu).\n",
        "* On the Colaboratory Pro GPU, training takes about 3-9 hours. Free GPUs may be slower.\n",
        "* By default, checkpoints will be saved every 300 steps with a maximum of 10 checkpoints.\n",
        "* Feel free to adjust these numbers depending on the frequency of saves you would like and the space on your drive.\n",
        "* If your Colaboratory runtime has stopped, re-run steps 1 through 9 to resume training from your most recent checkpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fT-8Koyvj46w"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QID5V8RzH7DR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b21531b4-781c-4ca7-9705-6c87abec63c0"
      },
      "source": [
        "if LATENT_VECTORS: \n",
        "  %cd /content/DDSP-48kHz-Stereo\n",
        "else:\n",
        "  %cd /content/DDSP-48kHz-Stereo-NoZ\n",
        "\n",
        "TRAIN_TFRECORD = '/content/data/train.tfrecord'\n",
        "TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
        "\n",
        "!python ddsp/ddsp_run.py \\\n",
        "  --mode=train \\\n",
        "  --alsologtostderr \\\n",
        "  --save_dir=\"$SAVE_DIR\" \\\n",
        "  --gin_file=models/solo_instrument.gin \\\n",
        "  --gin_file=datasets/tfrecord.gin \\\n",
        "  --gin_param=\"TFRecordProvider.file_pattern='$TRAIN_TFRECORD_FILEPATTERN'\" \\\n",
        "  --gin_param=\"TFRecordProvider.example_secs=$RENDER_SECONDS\" \\\n",
        "  --gin_param=\"Autoencoder.n_samples=$RENDER_SAMPLES\" \\\n",
        "  --gin_param=\"batch_size=8\" \\\n",
        "  --gin_param=\"train_util.train.num_steps=90000\" \\\n",
        "  --gin_param=\"train_util.train.steps_per_save=300\" \\\n",
        "  --gin_param=\"trainers.Trainer.checkpoints_to_keep=10\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvYjtUE-0055",
        "colab_type": "text"
      },
      "source": [
        "## **Step 10**--Timbre transfer imports\n",
        "\n",
        "Now it's time to render the final audio file with the aid of an audio primer file for timbre transfer. We'll start with some basic imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnTQi5YD0SSo",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "30c3a953-17ed-4f70-b568-2d0530c27474"
      },
      "source": [
        "if LATENT_VECTORS: \n",
        "  %cd /content/DDSP-48kHz-Stereo/ddsp\n",
        "else:\n",
        "  %cd /content/DDSP-48kHz-Stereo-NoZ/ddsp\n",
        "\n",
        "# Ignore a bunch of deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import copy\n",
        "import time\n",
        "import pydub\n",
        "import gin\n",
        "import crepe\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import core\n",
        "import spectral_ops\n",
        "from training import metrics\n",
        "from training import models\n",
        "from colab import colab_utils\n",
        "from colab.colab_utils import (auto_tune, detect_notes, fit_quantile_transform, get_tuning_factor, download, play, record, specplot, upload, DEFAULT_SAMPLE_RATE)\n",
        "from google.colab import files\n",
        "\n",
        "# Helper Functions\n",
        "sample_rate = 48000\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ho6xMa2JR6",
        "colab_type": "text"
      },
      "source": [
        "## **Step 11**--Process audio primer\n",
        "\n",
        "The key to transcending the sonic bounds of the dataset is the audio primer file. This file will graft its frequency and loudness information onto the rendered audio file, sort of like a vocoder. Then you can use the sliders in the \"Modify Conditioning\" section to further alter the rendered file.\n",
        "\n",
        "Put your audio primer files in the \"primers\" directory inside the directory you set as DRIVE_DIR. Input the file name of the primer you want to use on the line below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07r9M7ST1L2b",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d8c7aac3-bc69-4cb6-f98f-1b50f83f19cc"
      },
      "source": [
        "PRIMER_DIR = DRIVE_DIR + '/primers/'\n",
        "PRIMER_FILE =  \"OTO16S48a.wav\" #@param {type:\"string\"}\n",
        "\n",
        "# Check for .wav extension\n",
        "match = re.search(r'.wav', PRIMER_FILE)\n",
        "if match:\n",
        "  print ('')\n",
        "else:\n",
        "  PRIMER_FILE = PRIMER_FILE + \".wav\"\n",
        "\n",
        "PATH_TO_PRIMER = PRIMER_DIR + PRIMER_FILE\n",
        "\n",
        "from scipy.io.wavfile import read as read_audio\n",
        "from scipy.io.wavfile import write as write_audio\n",
        "\n",
        "primer_sample_rate, audio = read_audio(PATH_TO_PRIMER)\n",
        "\n",
        "# Setup the session.\n",
        "spectral_ops.reset_crepe()\n",
        "\n",
        "# Compute features.\n",
        "start_time = time.time()\n",
        "audio_features = metrics.compute_audio_features(audio)\n",
        "audio_features['loudness_dbM'] = audio_features['loudness_dbM'].astype(np.float32)\n",
        "audio_features['loudness_dbL'] = audio_features['loudness_dbL'].astype(np.float32)\n",
        "audio_features['loudness_dbR'] = audio_features['loudness_dbR'].astype(np.float32)\n",
        "audio_features_mod = None\n",
        "print('Audio features took %.1f seconds' % (time.time() - start_time))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zegk_zKNtu51",
        "colab_type": "text"
      },
      "source": [
        "## **Step 12**--Load most recent checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN5neGx21R7a",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "07eeaab4-4a8b-43f0-a99f-45eaeedf58bb"
      },
      "source": [
        "# Copy most recent checkpoint to \"ckpt\" folder\n",
        "%cd $DRIVE_DIR/ckpt/\n",
        "!rm *\n",
        "CHECKPOINT_ZIP = 'ckpt.zip'\n",
        "latest_checkpoint_fname = os.path.basename(tf.train.latest_checkpoint(SAVE_DIR))  + '*'\n",
        "!cd \"$SAVE_DIR\"\n",
        "!cd \"$SAVE_DIR\" && zip $CHECKPOINT_ZIP $latest_checkpoint_fname* operative_config-0.gin dataset_statistics.pkl\n",
        "!cp \"$SAVE_DIR/$CHECKPOINT_ZIP\" \"$DRIVE_DIR/ckpt/\"\n",
        "!unzip -o \"$CHECKPOINT_ZIP\"\n",
        "!rm \"$CHECKPOINT_ZIP\"\n",
        "%cd $SAVE_DIR\n",
        "!rm \"$CHECKPOINT_ZIP\"\n",
        "model_dir = DRIVE_DIR + '/ckpt/'\n",
        "gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "\n",
        "# Load the dataset statistics.\n",
        "DATASET_STATS = None\n",
        "dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')\n",
        "print(f'Loading dataset statistics from {dataset_stats_file}')\n",
        "try:\n",
        "  if tf.io.gfile.exists(dataset_stats_file):\n",
        "    with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
        "      DATASET_STATS = pickle.load(f)\n",
        "except Exception as err:\n",
        "  print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
        "\n",
        "# Parse gin config,\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
        "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
        "ckpt_name = ckpt_files[0].split('.')[0]\n",
        "ckpt = os.path.join(model_dir, ckpt_name)\n",
        "\n",
        "# Ensure dimensions and sampling rates are equal\n",
        "time_steps_train = gin.query_parameter('DefaultPreprocessor.time_steps')\n",
        "n_samples_train = RENDER_SAMPLES\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "time_steps = int(audio_features['audioL'].shape[1] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "# Trim all input vectors to correct lengths \n",
        "for key in ['f0_hzM', 'f0_hzL', 'f0_hzR', 'f0_confidenceM', 'f0_confidenceL', 'f0_confidenceR']:\n",
        "  audio_features[key] = audio_features[key][:time_steps]\n",
        "\n",
        "for key in ['loudness_dbM', 'loudness_dbL', 'loudness_dbR']:\n",
        "  audio_features[key] = audio_features[key][:, :time_steps]\n",
        "\n",
        "audio_features['audioM'] = audio_features['audioM'][:, :n_samples]\n",
        "audio_features['audioL'] = audio_features['audioL'][:, :n_samples]\n",
        "audio_features['audioR'] = audio_features['audioR'][:, :n_samples]\n",
        "\n",
        "# Set up the model just to predict audio given new conditioning\n",
        "model = models.Autoencoder()\n",
        "model.restore(ckpt)\n",
        "\n",
        "# Build model by running a batch through it.\n",
        "start_time = time.time()\n",
        "_ = model(audio_features, training=False)\n",
        "print('Restoring model took %.1f seconds' % (time.time() - start_time))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTw2WJpG-Sl5",
        "colab_type": "text"
      },
      "source": [
        "## **Step 13** (optional)--Modify Conditioning\n",
        "\n",
        "These models were not explicitly trained to perform timbre transfer, so they may sound unnatural if the incoming loudness and frequencies are very different then the training data (which will always be somewhat true). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcw1r3MO-fTB",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee97f5ce-f1a6-4768-d43f-1defdc7e11fb"
      },
      "source": [
        "#@markdown ## Note Detection\n",
        "\n",
        "#@markdown You can leave this at 1.0 for most cases\n",
        "threshold = 1 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
        "\n",
        "\n",
        "#@markdown ## Automatic\n",
        "\n",
        "ADJUST = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown Quiet parts without notes detected (dB)\n",
        "quiet = 30 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
        "\n",
        "#@markdown Force pitch to nearest note (amount)\n",
        "autotune = 0 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown ## Manual\n",
        "\n",
        "\n",
        "#@markdown Shift the pitch (octaves)\n",
        "pitch_shift =  0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "\n",
        "#@markdown Adjsut the overall loudness (dB)\n",
        "loudness_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "\n",
        "\n",
        "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
        "\n",
        "## Helper functions.\n",
        "def shift_ld(audio_features, ld_shiftL=0.0, ld_shiftR=0.0):\n",
        "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
        "  audio_features['loudness_dbL'] += ld_shiftL\n",
        "  audio_features['loudness_dbR'] += ld_shiftR\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, pitch_shiftL=0.0, pitch_shiftR=0.0):\n",
        "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
        "  audio_features['f0_hzL'] *= 2.0 ** (pitch_shiftL)\n",
        "  audio_features['f0_hzL'] = np.clip(audio_features['f0_hzL'], \n",
        "                                    0.0, \n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  audio_features['f0_hzR'] *= 2.0 ** (pitch_shiftR)\n",
        "  audio_features['f0_hzR'] = np.clip(audio_features['f0_hzR'], \n",
        "                                    0.0, \n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "mask_on = None\n",
        "\n",
        "if ADJUST and DATASET_STATS is not None:\n",
        "  # Detect sections that are \"on\".\n",
        "  mask_onL, note_on_valueL = detect_notes(audio_features['loudness_dbL'],\n",
        "                                        audio_features['f0_confidenceL'],\n",
        "                                        threshold)\n",
        "  \n",
        "  mask_onR, note_on_valueR = detect_notes(audio_features['loudness_dbR'],\n",
        "                                        audio_features['f0_confidenceR'],\n",
        "                                        threshold)\n",
        "\n",
        "  if np.any(mask_onL):\n",
        "    # Shift the pitch register.\n",
        "    target_mean_pitchL = DATASET_STATS['mean_pitchL']\n",
        "    target_mean_pitchR = DATASET_STATS['mean_pitchR']\n",
        "    pitchL = core.hz_to_midi(audio_features['f0_hzL'])\n",
        "    pitchR = core.hz_to_midi(audio_features['f0_hzR'])\n",
        "    pitchL = np.expand_dims(pitchL, axis=0)\n",
        "    pitchR = np.expand_dims(pitchR, axis=0)\n",
        "    mean_pitchL = np.mean(pitchL[mask_onL])\n",
        "    mean_pitchR = np.mean(pitchR[mask_onR])\n",
        "    p_diffL = target_mean_pitchL - mean_pitchL\n",
        "    p_diffR = target_mean_pitchR - mean_pitchR\n",
        "    p_diff_octaveL = p_diffL / 12.0\n",
        "    p_diff_octaveR = p_diffR / 12.0\n",
        "    round_fnL = np.floor if p_diff_octaveL > 1.5 else np.ceil\n",
        "    round_fnR = np.floor if p_diff_octaveR > 1.5 else np.ceil\n",
        "    p_diff_octaveL = round_fnL(p_diff_octaveL)\n",
        "    p_diff_octaveR = round_fnR(p_diff_octaveR)\n",
        "\n",
        "    audio_features_mod = shift_f0(audio_features_mod, p_diff_octaveL, p_diff_octaveR)\n",
        "\n",
        "    # Quantile shift the note_on parts.\n",
        "    _, loudness_normL = colab_utils.fit_quantile_transform(\n",
        "        audio_features['loudness_dbL'],\n",
        "        mask_onL,\n",
        "        inv_quantile=DATASET_STATS['quantile_transformL'])\n",
        "    \n",
        "    # Quantile shift the note_on parts.\n",
        "    _, loudness_normR = colab_utils.fit_quantile_transform(\n",
        "        audio_features['loudness_dbR'],\n",
        "        mask_onR,\n",
        "        inv_quantile=DATASET_STATS['quantile_transformR'])\n",
        "\n",
        "    # Turn down the note_off parts.\n",
        "    mask_offL = np.logical_not(mask_onL)\n",
        "    mask_offR = np.logical_not(mask_onR)\n",
        "    loudness_normL = np.squeeze(loudness_normL)\n",
        "    loudness_normR = np.squeeze(loudness_normR)\n",
        "    loudness_normL[np.squeeze(mask_offL)] -=  quiet * (1.0 - note_on_valueL[mask_offL])\n",
        "    loudness_normR[np.squeeze(mask_offR)] -=  quiet * (1.0 - note_on_valueR[mask_offR])\n",
        "    loudness_normL = np.reshape(loudness_normL, audio_features['loudness_dbL'].shape)\n",
        "    loudness_normR = np.reshape(loudness_normR, audio_features['loudness_dbR'].shape)\n",
        "    \n",
        "    audio_features_mod['loudness_dbL'] = loudness_normL\n",
        "    audio_features_mod['loudness_dbR'] = loudness_normR\n",
        "\n",
        "    # Auto-tune.\n",
        "    if autotune:\n",
        "      f0_midiL = np.array(core.hz_to_midi(audio_features_mod['f0_hzL']))\n",
        "      f0_midiR = np.array(core.hz_to_midi(audio_features_mod['f0_hzR']))\n",
        "      tuning_factorL = get_tuning_factor(f0_midiL, audio_features_mod['f0_confidenceL'], np.squeeze(mask_onL))\n",
        "      tuning_factorR = get_tuning_factor(f0_midiR, audio_features_mod['f0_confidenceR'], np.squeeze(mask_onR))\n",
        "      f0_midi_atL = auto_tune(f0_midiL, tuning_factorL, np.squeeze(mask_onL), amount=autotune)\n",
        "      f0_midi_atR = auto_tune(f0_midiR, tuning_factorR, np.squeeze(mask_onR), amount=autotune)\n",
        "      audio_features_mod['f0_hzL'] = core.midi_to_hz(f0_midi_atL)\n",
        "      audio_features_mod['f0_hzR'] = core.midi_to_hz(f0_midi_atR)\n",
        "\n",
        "  else:\n",
        "    print('\\nSkipping auto-adjust (no notes detected or ADJUST box empty).')\n",
        "\n",
        "else:\n",
        "  print('\\nSkipping auto-adujst (box not checked or no dataset statistics found).')\n",
        "\n",
        "# Manual Shifts.\n",
        "audio_features_mod = shift_ld(audio_features_mod, loudness_shift, loudness_shift)\n",
        "audio_features_mod = shift_f0(audio_features_mod, pitch_shift, pitch_shift)\n",
        "\n",
        "TRIM = -15\n",
        "\n",
        "# Plot Features.\n",
        "has_maskL = int(mask_onL is not None)\n",
        "n_plots = 3 if has_maskL else 2 \n",
        "figL, axesL = plt.subplots(nrows=n_plots, \n",
        "                      ncols=1, \n",
        "                      sharex=True,\n",
        "                      figsize=(2*n_plots, 8))\n",
        "\n",
        "if has_maskL:\n",
        "  ax = axesL[0]\n",
        "  ax.plot(np.ones_like(np.squeeze(mask_onL)[:TRIM]) * threshold, 'k:')\n",
        "  ax.plot(np.squeeze(note_on_valueL)[:TRIM])\n",
        "  ax.plot(np.squeeze(mask_onL)[:TRIM])\n",
        "  ax.set_ylabel('Note-on Mask--Left')\n",
        "  ax.set_xlabel('Time step [frame]--Left')\n",
        "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
        "\n",
        "ax = axesL[0 + has_maskL]\n",
        "ax.plot(np.squeeze(audio_features['loudness_dbL'])[:TRIM])\n",
        "ax.plot(np.squeeze(audio_features_mod['loudness_dbL'])[:TRIM])\n",
        "ax.set_ylabel('loudness_db--Left')\n",
        "ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axesL[1 + has_maskL]\n",
        "ax.plot(librosa.hz_to_midi(np.squeeze(audio_features['f0_hzL'])[:TRIM]))\n",
        "ax.plot(librosa.hz_to_midi(np.squeeze(audio_features_mod['f0_hzL'])[:TRIM]))\n",
        "ax.set_ylabel('f0 [midi]--Left')\n",
        "_ = ax.legend(['Original','Adjusted'])\n",
        "\n",
        "has_maskR = int(mask_onR is not None)\n",
        "n_plots = 3 if has_maskR else 2 \n",
        "figR, axesR = plt.subplots(nrows=n_plots, \n",
        "                      ncols=1, \n",
        "                      sharex=True,\n",
        "                      figsize=(2*n_plots, 8))\n",
        "\n",
        "if has_maskR:\n",
        "  ax = axesR[0]\n",
        "  ax.plot(np.ones_like(np.squeeze(mask_onR)[:TRIM]) * threshold, 'k:')\n",
        "  ax.plot(np.squeeze(note_on_valueR)[:TRIM])\n",
        "  ax.plot(np.squeeze(mask_onR)[:TRIM])\n",
        "  ax.set_ylabel('Note-on Mask--Right')\n",
        "  ax.set_xlabel('Time step [frame]--Right')\n",
        "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
        "\n",
        "ax = axesR[0 + has_maskR]\n",
        "ax.plot(np.squeeze(audio_features['loudness_dbR'])[:TRIM])\n",
        "ax.plot(np.squeeze(audio_features_mod['loudness_dbR'])[:TRIM])\n",
        "ax.set_ylabel('loudness_db--Right')\n",
        "ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axesR[1 + has_maskR]\n",
        "ax.plot(librosa.hz_to_midi(np.squeeze(audio_features['f0_hzR'])[:TRIM]))\n",
        "ax.plot(librosa.hz_to_midi(np.squeeze(audio_features_mod['f0_hzR'])[:TRIM]))\n",
        "ax.set_ylabel('f0 [midi]--Right')\n",
        "_ = ax.legend(['Original','Adjusted'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG6d-jw8pG42",
        "colab_type": "text"
      },
      "source": [
        "## **Step 14**--Render audio\n",
        "\n",
        "After running this cell, your final rendered file should be downloaded automatically. If not, look for it in the \"audio_output/normalized\" directory inside the directory you set as DRIVE_DIR. There are also unnormalized stereo and mono files in the \"audio_output\" directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ko4WGOIjRkN",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d07c3678-13c4-4a1c-d3b6-64b84c2a6186"
      },
      "source": [
        "%cd $AUDIO_OUTPUT_DIR\n",
        "!mkdir -p normalized\n",
        "!rm normalized/*\n",
        "\n",
        "af = audio_features if audio_features_mod is None else audio_features_mod\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "audio_genM, audio_genL, audio_genR = model(af, training=False)\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "audio_genL = np.expand_dims(np.squeeze(audio_genL.numpy()), axis=1)\n",
        "audio_genR = np.expand_dims(np.squeeze(audio_genR.numpy()), axis=1)\n",
        "audio_genS = np.concatenate((audio_genL, audio_genR), axis=1)\n",
        "audio_genM = np.expand_dims(np.squeeze(audio_genM.numpy()), axis=1)\n",
        "\n",
        "write_audio(\"renderS.wav\", 48000, audio_genS)\n",
        "write_audio(\"renderM.wav\", 48000, audio_genM)\n",
        "\n",
        "!ffmpeg-normalize renderS.wav -o normalized/render.wav -t -15 -ar 48000\n",
        "\n",
        "colab_utils.download(\"normalized/render.wav\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etgf80MPGWQz",
        "colab_type": "text"
      },
      "source": [
        "## **Step 15** (optional)--Download your model for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbx-cZJOD4UR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "99845fa4-9adc-445c-b35f-3ce2ff411a72"
      },
      "source": [
        "%cd $CKPT_OUTPUT_DIR\n",
        "!zip -r checkpoint.zip *\n",
        "colab_utils.download('checkpoint.zip')\n",
        "!rm checkpoint.zip"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
