{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddsp_train_and_timbre_transfer_48kHz_stereo.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJd3dlOCStH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DemonFlexCouncil/DDSP-48kHz-Stereo/blob/master/ddsp/colab/ddsp_train_and_timbre_transfer_48kHz_stereo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMqWDc_m6rUC"
      },
      "source": [
        "\n",
        "##### Copyright 2020 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNhgka4UKNjf"
      },
      "source": [
        "# Copyright 2020 Google LLC. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpXo6phTiOQM"
      },
      "source": [
        "# Train & Timbre Transfer--DDSP Autoencoder on GPU--48kHz/Stereo\n",
        "\n",
        "Made by [Google Magenta](https://magenta.tensorflow.org/)--altered by [Flex Council](https://soundcloud.com/flexcouncil)\n",
        "\n",
        "**A Little Background**\n",
        "\n",
        "A producer friend of mine turned me on to Magenta’s DDSP, and I’m glad he did. In my mind it represents the way forward for AI music. Finally we have a glimpse inside the black box, with access to musical parameters as well as neural net hyperparameters. And DDSP leverages decades of studio knowledge by utilizing traditional processors like synthesizers and effects. One can envision a time when DDSP-like elements will sit at the heart of production DAWs.\n",
        "\n",
        "DDSP will accept most audio sample rates and formats. However, native 48 kHz/stereo datasets and primers will sound best. Output files are always 48 kHz/stereo. You can upload datasets and primers via Google Drive.\n",
        "\n",
        "According to Magenta’s paper, this algorithm was intended as proof of concept, but I wanted to bend it more towards a tool for producers. I bumped the sample rate up to 48 kHz and made it stereo. I also introduced a variable render length so you can feed it loops or phrases. However, there are limits to this parameter. The total number of samples in your render length (number of seconds * 48000) must be evenly divisible by 800. In practice, this means using round-numbered or highly-divisible tempos (105, 96, 90, 72, 50…) or using material that does not depend on tempo.\n",
        "\n",
        "Also note that longer render times may require a smaller batch size, which is currently set at 8 for a 4-second render. This may diminish audio quality, so use shorter render times if at all possible.\n",
        "\n",
        "You can train with or without latent vectors, z(t). There is a tradeoff here. The lack of latent vectors allows for more pronounced shifts in the “Modify Conditioning” section, but the rendered audio sounds cloudier. The default mode is latent vectors.\n",
        "\n",
        "The dataset and primer files must be WAVE format, stereo, and 48 kHz. Most DAWs and audio editors have a 48 kHz export option, including the free [Audacity](https://www.audacityteam.org/). There appears to be a lower limit on the total size of the dataset, somewhere around 5MB. Smaller sizes may produce blank TFRecords (0 bytes.) Also, Colaboratory may throw memory errors if it encounters a large single audio file for the dataset—cut the file into smaller pieces if this happens.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/ddsp/additive_diagram/ddsp_autoencoder.png\" alt=\"DDSP Autoencoder figure\" width=\"700\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXjcauVRB48S"
      },
      "source": [
        "**Note that bash commands are prefixed with a `!` inside of Colaboratory, but you would leave them out if running directly in a terminal.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn7CQ4GQizHy"
      },
      "source": [
        "## **Step 1**--Install dependencies\n",
        "First we install the required dependencies with `pip` (takes about 5 minutes.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "VxPuPR0j5Gs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f700b0-04be-4ec7-8696-7de48c52f9cc"
      },
      "source": [
        "!pip install mir_eval\n",
        "!pip install apache_beam\n",
        "!pip install crepe\n",
        "!pip install pydub\n",
        "!pip3 install ffmpeg-normalize\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6MXUbL6KeMn"
      },
      "source": [
        "## **Step 2**--Login and mount your Google Drive\n",
        "\n",
        "This will require authentication of your Google account. After authentication, you should be able to see your Drive in the file browser on the left panel--click the folder icon to make it visible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m33xuTjEKazJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6114f651-97c0-4369-bfd3-be784f3162aa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT9ASvNq4999"
      },
      "source": [
        "## **Step 3**--Set render length\n",
        "\n",
        "Determines the length of audio slices for training and resynthesis. Decimals are OK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9p-35hx22sL"
      },
      "source": [
        "RENDER_SECONDS =  4.0#@param {type:\"number\", min:1, max:10}\n",
        "RENDER_SAMPLES = int(RENDER_SECONDS * 48000)\n",
        "\n",
        "if ((RENDER_SAMPLES % 800) != 0):\n",
        "  raise ValueError(\"Number of samples at 48kHz must be divisble by 800.\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECATHBbTvY3H"
      },
      "source": [
        "## **Step 4**--Latent vectors mode\n",
        "\n",
        "Uncheck the box to train without z(t)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0KXh3D-vlxo"
      },
      "source": [
        "LATENT_VECTORS = True #@param{type:\"boolean\"}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bICWo4uclNCr"
      },
      "source": [
        "## **Step 5**--Set your training directory on Drive and get the DDSP repository from Github\n",
        "\n",
        "In the left panel, pick a folder on your Drive where you want to upload audio files and store checkpoints. Then right-click on the folder and select \"Copy path.\" Enter the path below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BndBqU9mlo5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f93c1f0-8a1a-46c7-c389-beeb967e7df8"
      },
      "source": [
        "DRIVE_DIR = Enter training directory here#@param {type:\"string\"}\n",
        "\n",
        "if LATENT_VECTORS:\n",
        "  !git clone https://github.com/FlexCouncil/DDSP-48kHz-Stereo.git\n",
        "else:\n",
        "  !git clone https://github.com/FlexCouncil/DDSP-48kHz-Stereo-NoZ.git\n",
        "\n",
        "AUDIO_DIR = '/content/data/audio'\n",
        "!mkdir -p $AUDIO_DIR\n",
        "AUDIO_FILEPATTERN = AUDIO_DIR + '/*'\n",
        "AUDIO_INPUT_DIR = DRIVE_DIR + '/audio_input'\n",
        "AUDIO_OUTPUT_DIR = DRIVE_DIR + '/audio_output'\n",
        "CKPT_OUTPUT_DIR = DRIVE_DIR + '/ckpt'\n",
        "SAVE_DIR = os.path.join(DRIVE_DIR, 'model')\n",
        "\n",
        "%cd $DRIVE_DIR\n",
        "!mkdir -p audio_input audio_output ckpt data model primer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4vmxpj1LC7m"
      },
      "source": [
        "## **Step 6**--Upload your audio files to Drive and create a TFRecord dataset\n",
        " In your Drive interface, put your training audio files in the \"audio_input\" directory, which is inside the directory you set as DRIVE_DIR. The algorithm typically works well with audio from a single acoustic environment.\n",
        "\n",
        "Preprocessing involves inferring the fundamental frequency (the main pitch) with [CREPE](http://github.com/marl/crepe), and computing the loudness. These features will then be stored in sharded [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) files for easier loading. Depending on the amount of input audio, this process usually takes a few minutes with a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARQyA8m0q9vb",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c4f360-c1ab-43a6-8f79-e625e8806476"
      },
      "source": [
        "audio_files = glob.glob(os.path.join(AUDIO_INPUT_DIR, '*.wav'))\n",
        "\n",
        "for fname in audio_files:\n",
        "  target_name = os.path.join(AUDIO_DIR,\n",
        "                             os.path.basename(fname).replace(' ', '_'))\n",
        "  print('Copying {} to {}'.format(fname, target_name))\n",
        "  !cp \"$fname\" $target_name\n",
        "\n",
        "TRAIN_TFRECORD = '/content/data/train.tfrecord'\n",
        "TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
        "\n",
        "drive_data_dir = os.path.join(DRIVE_DIR, 'data')\n",
        "drive_dataset_files = glob.glob(drive_data_dir + '/*')\n",
        "\n",
        "# Make a new dataset.\n",
        "if not glob.glob(AUDIO_FILEPATTERN):\n",
        "  raise ValueError('No audio files found. Please use the previous cell to '\n",
        "                    'upload.')\n",
        "\n",
        "if LATENT_VECTORS:\n",
        "  !python /content/DDSP-48kHz-Stereo/ddsp/training/data_preparation/prepare_tfrecord.py \\\n",
        "    --input_audio_filepatterns=$AUDIO_FILEPATTERN \\\n",
        "    --output_tfrecord_path=$TRAIN_TFRECORD \\\n",
        "    --num_shards=10 \\\n",
        "    --example_secs=$RENDER_SECONDS \\\n",
        "    --alsologtostderr\n",
        "else:\n",
        "  !python /content/DDSP-48kHz-Stereo-NoZ/ddsp/training/data_preparation/prepare_tfrecord.py \\\n",
        "    --input_audio_filepatterns=$AUDIO_FILEPATTERN \\\n",
        "    --output_tfrecord_path=$TRAIN_TFRECORD \\\n",
        "    --num_shards=10 \\\n",
        "    --example_secs=$RENDER_SECONDS \\\n",
        "    --alsologtostderr\n",
        "\n",
        "TRAIN_TFRECORD_DIR = DRIVE_DIR + '/data'\n",
        "TRAIN_TFRECORD_DIR = TRAIN_TFRECORD_DIR.replace(\"My Drive\", \"My\\ Drive\")\n",
        "!cp $TRAIN_TFRECORD_FILEPATTERN $TRAIN_TFRECORD_DIR"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4toX-D-AYZL"
      },
      "source": [
        "## **Step 7**--Save dataset statistics for timbre transfer\n",
        "\n",
        "We now compute quantile normalization, which helps to match the loudness of the timbre transfer inputs to the loudness of the dataset. This information is saved in a pickle file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp_c8P0xApY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b6301d-3007-41d3-ee09-202497f465e4"
      },
      "source": [
        "if LATENT_VECTORS:\n",
        "  %cd /content/DDSP-48kHz-Stereo/ddsp/\n",
        "else:\n",
        "  %cd /content/DDSP-48kHz-Stereo-NoZ/ddsp/\n",
        "\n",
        "from colab import colab_utils\n",
        "from training import data\n",
        "\n",
        "TRAIN_TFRECORD = '/content/data/train.tfrecord'\n",
        "TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
        "\n",
        "data_provider = data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN, example_secs=RENDER_SECONDS)\n",
        "dataset = data_provider.get_dataset(shuffle=False)\n",
        "\n",
        "PICKLE_FILE_PATH = os.path.join(SAVE_DIR, 'dataset_statistics.pkl')\n",
        "\n",
        "colab_utils.save_dataset_statistics(data_provider, PICKLE_FILE_PATH, batch_size=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoCFCynuaiK_"
      },
      "source": [
        "## **Step 8**--Train model\n",
        "\n",
        "DDSP was designed to model a single instrument, but you can also get interesting results by training it on sparse multi-timbral material.\n",
        "\n",
        "Note that  [gin configuration files](https://github.com/google/gin-config) specify parameters for the both the model architecture (solo_instrument.gin) and the dataset (tfrecord.gin.) These parameters can be overriden in the run script below (!python ddsp/ddsp_run.py).\n",
        "\n",
        "### Training Notes:\n",
        "* Models typically perform well when the loss drops to the range of ~7.0-8.5.\n",
        "* Depending on the dataset, this can take anywhere from 30k-90k training steps.\n",
        "* The default number of training steps is 90k, but you can stop training at any time (select \"Interrupt execution\" from the \"Runtime\" menu.) You can also adjust this parameter in the code below (train_util.train.num_steps.)\n",
        "* With a relatively fast GPU, training takes about 3-9 hours. Free GPUs may be slower.\n",
        "* By default, checkpoints will be saved every 300 steps with a maximum of 10 checkpoints. These parameters can be adjusted below with \"train_util.train.steps_per_save\" and \"trainers.Tra_stepsiner.checkpoints_to_keep,\" respectively.\n",
        "* If your Colaboratory runtime has stopped, re-run steps 1 through 8 to resume training from your most recent checkpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT-8Koyvj46w"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QID5V8RzH7DR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d87bec69-1660-4ec7-b82e-d7179e41c82a"
      },
      "source": [
        "if LATENT_VECTORS:\n",
        "  %cd /content/DDSP-48kHz-Stereo\n",
        "else:\n",
        "  %cd /content/DDSP-48kHz-Stereo-NoZ\n",
        "\n",
        "TRAIN_TFRECORD = '/content/data/train.tfrecord'\n",
        "TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
        "\n",
        "!python ddsp/ddsp_run.py \\\n",
        "  --mode=train \\\n",
        "  --alsologtostderr \\\n",
        "  --save_dir=\"$SAVE_DIR\" \\\n",
        "  --gin_file=models/solo_instrument.gin \\\n",
        "  --gin_file=datasets/tfrecord.gin \\\n",
        "  --gin_param=\"TFRecordProvider.file_pattern='/content/data/train.tfrecord*'\" \\\n",
        "  --gin_param=\"TFRecordProvider.example_secs=$RENDER_SECONDS\" \\\n",
        "  --gin_param=\"Autoencoder.n_samples=$RENDER_SAMPLES\" \\\n",
        "  --gin_param=\"batch_size=2\" \\\n",
        "  --gin_param=\"train_util.train.num_steps=90000\" \\\n",
        "  --gin_param=\"train_util.train.steps_per_save=1\" \\\n",
        "  --gin_param=\"trainers.Tra_stepsiner.checkpoints_to_keep=10\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvYjtUE-0055"
      },
      "source": [
        "## **Step 9**--Timbre transfer imports\n",
        "\n",
        "Now we start the inference process with importing the necessary timbre transfer packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnTQi5YD0SSo",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6a5106-3da4-4bfe-8145-62d26ef6b627"
      },
      "source": [
        "if LATENT_VECTORS:\n",
        "  %cd /content/DDSP-48kHz-Stereo/ddsp\n",
        "else:\n",
        "  %cd /content/DDSP-48kHz-Stereo-NoZ/ddsp\n",
        "\n",
        "# Ignore a bunch of deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import copy\n",
        "import time\n",
        "import pydub\n",
        "import gin\n",
        "import crepe\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import core\n",
        "import spectral_ops\n",
        "from training import metrics\n",
        "from training import models\n",
        "from colab import colab_utils\n",
        "from colab.colab_utils import (auto_tune, detect_notes, fit_quantile_transform, get_tuning_factor, download, play, record, specplot, upload, DEFAULT_SAMPLE_RATE)\n",
        "from google.colab import files\n",
        "\n",
        "# Helper Functions\n",
        "sample_rate = 48000\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ho6xMa2JR6"
      },
      "source": [
        "## **Step 10**--Process audio primer\n",
        "\n",
        "The \"primer\" file will graft its frequency and loudness information onto the rendered audio file, similar to a vocoder. You can then use the sliders in the \"Modify Conditioning\" section to further alter the rendered file.\n",
        "\n",
        "Put your audio primer file in the \"primer\" directory, which is inside the directory you set as DRIVE_DIR. Enter the file name of the primer on the line below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07r9M7ST1L2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9451d9c-71bc-4b15-a0c1-13f73be95df9"
      },
      "source": [
        "PRIMER_DIR = DRIVE_DIR + '/primer/'\n",
        "PRIMER_FILE =  Enter primer file name here#@param {type:\"string\"}\n",
        "\n",
        "# Check for .wav extension\n",
        "match = re.search(r'.wav', PRIMER_FILE)\n",
        "if match:\n",
        "  print ('')\n",
        "else:\n",
        "  PRIMER_FILE = PRIMER_FILE + \".wav\"\n",
        "\n",
        "PATH_TO_PRIMER = PRIMER_DIR + PRIMER_FILE\n",
        "\n",
        "from scipy.io.wavfile import read as read_audio\n",
        "from scipy.io.wavfile import write as write_audio\n",
        "\n",
        "primer_sample_rate, audio = read_audio(PATH_TO_PRIMER)\n",
        "\n",
        "# Setup the session.\n",
        "spectral_ops.reset_crepe()\n",
        "\n",
        "# Compute features.\n",
        "start_time = time.time()\n",
        "audio_features = metrics.compute_audio_features(audio)\n",
        "audio_features['loudness_dbM'] = audio_features['loudness_dbM'].astype(np.float32)\n",
        "audio_features['loudness_dbL'] = audio_features['loudness_dbL'].astype(np.float32)\n",
        "audio_features['loudness_dbR'] = audio_features['loudness_dbR'].astype(np.float32)\n",
        "audio_features_mod = None\n",
        "print('Audio features took %.1f seconds' % (time.time() - start_time))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zegk_zKNtu51"
      },
      "source": [
        "## **Step 11**--Load the most recent checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN5neGx21R7a",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b120197-b484-46e5-ab88-d1f3bfa7dd36"
      },
      "source": [
        "import collections\n",
        "import collections.abc\n",
        "\n",
        "# Patch collections module\n",
        "collections.Iterable = collections.abc.Iterable\n",
        "collections.Mapping = collections.abc.Mapping\n",
        "collections.MutableMapping = collections.abc.MutableMapping\n",
        "collections.MutableSet = collections.abc.MutableSet\n",
        "collections.Set = collections.abc.Set\n",
        "\n",
        "# Copy most recent checkpoint to \"ckpt\" folder\n",
        "%cd $DRIVE_DIR/ckpt/\n",
        "!rm *\n",
        "CHECKPOINT_ZIP = 'ckpt.zip'\n",
        "latest_checkpoint_fname = os.path.basename(tf.train.latest_checkpoint(SAVE_DIR))  + '*'\n",
        "!cd \"$SAVE_DIR\"\n",
        "!cd \"$SAVE_DIR\" && zip $CHECKPOINT_ZIP $latest_checkpoint_fname* operative_config-0.gin dataset_statistics.pkl\n",
        "!cp \"$SAVE_DIR/$CHECKPOINT_ZIP\" \"$DRIVE_DIR/ckpt/\"\n",
        "!unzip -o \"$CHECKPOINT_ZIP\"\n",
        "!rm \"$CHECKPOINT_ZIP\"\n",
        "%cd $SAVE_DIR\n",
        "!rm \"$CHECKPOINT_ZIP\"\n",
        "model_dir = DRIVE_DIR + '/ckpt/'\n",
        "gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "\n",
        "# Load the dataset statistics.\n",
        "DATASET_STATS = None\n",
        "dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')\n",
        "print(f'Loading dataset statistics from {dataset_stats_file}')\n",
        "try:\n",
        "  if tf.io.gfile.exists(dataset_stats_file):\n",
        "    with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
        "      DATASET_STATS = pickle.load(f)\n",
        "except Exception as err:\n",
        "  print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
        "\n",
        "# Parse gin config,\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
        "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
        "ckpt_name = ckpt_files[0].split('.')[0]\n",
        "ckpt = os.path.join(model_dir, ckpt_name)\n",
        "\n",
        "# Ensure dimensions and sampling rates are equal\n",
        "time_steps_train = gin.query_parameter('DefaultPreprocessor.time_steps')\n",
        "n_samples_train = RENDER_SAMPLES\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "time_steps = int(audio_features['audioL'].shape[1] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "# Trim all input vectors to correct lengths\n",
        "for key in ['f0_hzM', 'f0_hzL', 'f0_hzR', 'f0_confidenceM', 'f0_confidenceL', 'f0_confidenceR']:\n",
        "  audio_features[key] = audio_features[key][:time_steps]\n",
        "\n",
        "for key in ['loudness_dbM', 'loudness_dbL', 'loudness_dbR']:\n",
        "  audio_features[key] = audio_features[key][:, :time_steps]\n",
        "\n",
        "audio_features['audioM'] = audio_features['audioM'][:, :n_samples]\n",
        "audio_features['audioL'] = audio_features['audioL'][:, :n_samples]\n",
        "audio_features['audioR'] = audio_features['audioR'][:, :n_samples]\n",
        "\n",
        "# Set up the model just to predict audio given new conditioning\n",
        "model = models.Autoencoder()\n",
        "model.restore(ckpt)\n",
        "\n",
        "# Build model by running a batch through it.\n",
        "start_time = time.time()\n",
        "_ = model(audio_features, training=False)\n",
        "print('Restoring model took %.1f seconds' % (time.time() - start_time))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTw2WJpG-Sl5"
      },
      "source": [
        "## **Step 12** (optional)--Modify Conditioning\n",
        "\n",
        "Here you can adjust the parameters affecting the final render."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcw1r3MO-fTB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82f1d5b6-0075-4c08-aeca-c844c7c6d23c"
      },
      "source": [
        "#@markdown ## Note Detection\n",
        "\n",
        "#@markdown You can leave this at 1.0 for most cases\n",
        "threshold = 1.23 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
        "\n",
        "\n",
        "#@markdown ## Automatic\n",
        "\n",
        "ADJUST = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown Quiet parts without notes detected (dB)\n",
        "quiet = 36 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
        "\n",
        "#@markdown Force pitch to nearest note (amount)\n",
        "autotune = 0.2 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown ## Manual\n",
        "\n",
        "\n",
        "#@markdown Shift the pitch (octaves)\n",
        "pitch_shift =  1 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "\n",
        "#@markdown Adjsut the overall loudness (dB)\n",
        "loudness_shift = -6 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "\n",
        "\n",
        "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
        "\n",
        "## Helper functions.\n",
        "def shift_ld(audio_features, ld_shiftL=0.0, ld_shiftR=0.0):\n",
        "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
        "  audio_features['loudness_dbL'] += ld_shiftL\n",
        "  audio_features['loudness_dbR'] += ld_shiftR\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, pitch_shiftL=0.0, pitch_shiftR=0.0):\n",
        "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
        "  audio_features['f0_hzL'] *= 2.0 ** (pitch_shiftL)\n",
        "  audio_features['f0_hzL'] = np.clip(audio_features['f0_hzL'],\n",
        "                                    0.0,\n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  audio_features['f0_hzR'] *= 2.0 ** (pitch_shiftR)\n",
        "  audio_features['f0_hzR'] = np.clip(audio_features['f0_hzR'],\n",
        "                                    0.0,\n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "mask_on = None\n",
        "\n",
        "if ADJUST and DATASET_STATS is not None:\n",
        "  # Detect sections that are \"on\".\n",
        "  mask_onL, note_on_valueL = detect_notes(audio_features['loudness_dbL'],\n",
        "                                        audio_features['f0_confidenceL'],\n",
        "                                        threshold)\n",
        "\n",
        "  mask_onR, note_on_valueR = detect_notes(audio_features['loudness_dbR'],\n",
        "                                        audio_features['f0_confidenceR'],\n",
        "                                        threshold)\n",
        "\n",
        "  if np.any(mask_onL):\n",
        "    # Shift the pitch register.\n",
        "    target_mean_pitchL = DATASET_STATS['mean_pitchL']\n",
        "    target_mean_pitchR = DATASET_STATS['mean_pitchR']\n",
        "    pitchL = core.hz_to_midi(audio_features['f0_hzL'])\n",
        "    pitchR = core.hz_to_midi(audio_features['f0_hzR'])\n",
        "    pitchL = np.expand_dims(pitchL, axis=0)\n",
        "    pitchR = np.expand_dims(pitchR, axis=0)\n",
        "    mean_pitchL = np.mean(pitchL[mask_onL])\n",
        "    mean_pitchR = np.mean(pitchR[mask_onR])\n",
        "    p_diffL = target_mean_pitchL - mean_pitchL\n",
        "    p_diffR = target_mean_pitchR - mean_pitchR\n",
        "    p_diff_octaveL = p_diffL / 12.0\n",
        "    p_diff_octaveR = p_diffR / 12.0\n",
        "    round_fnL = np.floor if p_diff_octaveL > 1.5 else np.ceil\n",
        "    round_fnR = np.floor if p_diff_octaveR > 1.5 else np.ceil\n",
        "    p_diff_octaveL = round_fnL(p_diff_octaveL)\n",
        "    p_diff_octaveR = round_fnR(p_diff_octaveR)\n",
        "\n",
        "    audio_features_mod = shift_f0(audio_features_mod, p_diff_octaveL, p_diff_octaveR)\n",
        "\n",
        "    # Quantile shift the note_on parts.\n",
        "    _, loudness_normL = colab_utils.fit_quantile_transform(\n",
        "        audio_features['loudness_dbL'],\n",
        "        mask_onL,\n",
        "        inv_quantile=DATASET_STATS['quantile_transformL'])\n",
        "\n",
        "    # Quantile shift the note_on parts.\n",
        "    _, loudness_normR = colab_utils.fit_quantile_transform(\n",
        "        audio_features['loudness_dbR'],\n",
        "        mask_onR,\n",
        "        inv_quantile=DATASET_STATS['quantile_transformR'])\n",
        "\n",
        "    # Turn down the note_off parts.\n",
        "    mask_offL = np.logical_not(mask_onL)\n",
        "    mask_offR = np.logical_not(mask_onR)\n",
        "    loudness_normL = np.squeeze(loudness_normL)\n",
        "    loudness_normR = np.squeeze(loudness_normR)\n",
        "    loudness_normL[np.squeeze(mask_offL)] -=  quiet * (1.0 - note_on_valueL[mask_offL])\n",
        "    loudness_normR[np.squeeze(mask_offR)] -=  quiet * (1.0 - note_on_valueR[mask_offR])\n",
        "    loudness_normL = np.reshape(loudness_normL, audio_features['loudness_dbL'].shape)\n",
        "    loudness_normR = np.reshape(loudness_normR, audio_features['loudness_dbR'].shape)\n",
        "\n",
        "    audio_features_mod['loudness_dbL'] = loudness_normL\n",
        "    audio_features_mod['loudness_dbR'] = loudness_normR\n",
        "\n",
        "    # Auto-tune.\n",
        "    if autotune:\n",
        "      f0_midiL = np.array(core.hz_to_midi(audio_features_mod['f0_hzL']))\n",
        "      f0_midiR = np.array(core.hz_to_midi(audio_features_mod['f0_hzR']))\n",
        "      tuning_factorL = get_tuning_factor(f0_midiL, audio_features_mod['f0_confidenceL'], np.squeeze(mask_onL))\n",
        "      tuning_factorR = get_tuning_factor(f0_midiR, audio_features_mod['f0_confidenceR'], np.squeeze(mask_onR))\n",
        "      f0_midi_atL = auto_tune(f0_midiL, tuning_factorL, np.squeeze(mask_onL), amount=autotune)\n",
        "      f0_midi_atR = auto_tune(f0_midiR, tuning_factorR, np.squeeze(mask_onR), amount=autotune)\n",
        "      audio_features_mod['f0_hzL'] = core.midi_to_hz(f0_midi_atL)\n",
        "      audio_features_mod['f0_hzR'] = core.midi_to_hz(f0_midi_atR)\n",
        "\n",
        "  else:\n",
        "    print('\\nSkipping auto-adjust (no notes detected or ADJUST box empty).')\n",
        "\n",
        "else:\n",
        "  print('\\nSkipping auto-adujst (box not checked or no dataset statistics found).')\n",
        "\n",
        "# Manual Shifts.\n",
        "audio_features_mod = shift_ld(audio_features_mod, loudness_shift, loudness_shift)\n",
        "audio_features_mod = shift_f0(audio_features_mod, pitch_shift, pitch_shift)\n",
        "\n",
        "TRIM = -15\n",
        "\n",
        "# Plot Features.\n",
        "has_maskL = int(mask_onL is not None)\n",
        "n_plots = 3 if has_maskL else 2\n",
        "figL, axesL = plt.subplots(nrows=n_plots,\n",
        "                      ncols=1,\n",
        "                      sharex=True,\n",
        "                      figsize=(2*n_plots, 8))\n",
        "\n",
        "if has_maskL:\n",
        "  ax = axesL[0]\n",
        "  ax.plot(np.ones_like(np.squeeze(mask_onL)[:TRIM]) * threshold, 'k:')\n",
        "  ax.plot(np.squeeze(note_on_valueL)[:TRIM])\n",
        "  ax.plot(np.squeeze(mask_onL)[:TRIM])\n",
        "  ax.set_ylabel('Note-on Mask--Left')\n",
        "  ax.set_xlabel('Time step [frame]--Left')\n",
        "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
        "\n",
        "ax = axesL[0 + has_maskL]\n",
        "ax.plot(np.squeeze(audio_features['loudness_dbL'])[:TRIM])\n",
        "ax.plot(np.squeeze(audio_features_mod['loudness_dbL'])[:TRIM])\n",
        "ax.set_ylabel('loudness_db--Left')\n",
        "ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axesL[1 + has_maskL]\n",
        "ax.plot(librosa.hz_to_midi(np.squeeze(audio_features['f0_hzL'])[:TRIM]))\n",
        "ax.plot(librosa.hz_to_midi(np.squeeze(audio_features_mod['f0_hzL'])[:TRIM]))\n",
        "ax.set_ylabel('f0 [midi]--Left')\n",
        "_ = ax.legend(['Original','Adjusted'])\n",
        "\n",
        "has_maskR = int(mask_onR is not None)\n",
        "n_plots = 3 if has_maskR else 2\n",
        "figR, axesR = plt.subplots(nrows=n_plots,\n",
        "                      ncols=1,\n",
        "                      sharex=True,\n",
        "                      figsize=(2*n_plots, 8))\n",
        "\n",
        "if has_maskR:\n",
        "  ax = axesR[0]\n",
        "  ax.plot(np.ones_like(np.squeeze(mask_onR)[:TRIM]) * threshold, 'k:')\n",
        "  ax.plot(np.squeeze(note_on_valueR)[:TRIM])\n",
        "  ax.plot(np.squeeze(mask_onR)[:TRIM])\n",
        "  ax.set_ylabel('Note-on Mask--Right')\n",
        "  ax.set_xlabel('Time step [frame]--Right')\n",
        "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
        "\n",
        "ax = axesR[0 + has_maskR]\n",
        "ax.plot(np.squeeze(audio_features['loudness_dbR'])[:TRIM])\n",
        "ax.plot(np.squeeze(audio_features_mod['loudness_dbR'])[:TRIM])\n",
        "ax.set_ylabel('loudness_db--Right')\n",
        "ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axesR[1 + has_maskR]\n",
        "ax.plot(librosa.hz_to_midi(np.squeeze(audio_features['f0_hzR'])[:TRIM]))\n",
        "ax.plot(librosa.hz_to_midi(np.squeeze(audio_features_mod['f0_hzR'])[:TRIM]))\n",
        "ax.set_ylabel('f0 [midi]--Right')\n",
        "_ = ax.legend(['Original','Adjusted'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG6d-jw8pG42"
      },
      "source": [
        "## **Step 13**--Render audio\n",
        "\n",
        "After running this cell, your final rendered file should be downloaded automatically. If it doesn't, look for it in the \"audio_output/normalized\" directory, which is inside the directory you set as DRIVE_DIR. You can also download unnormalized stereo and mono renders from the \"audio_output\" directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ko4WGOIjRkN",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "44b33524-8eff-43f8-ef58-5f6c6c0cfce2"
      },
      "source": [
        "%cd $AUDIO_OUTPUT_DIR\n",
        "!mkdir -p normalized\n",
        "!rm normalized/*\n",
        "\n",
        "af = audio_features if audio_features_mod is None else audio_features_mod\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "audio_genM, audio_genL, audio_genR = model(af, training=False)\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "audio_genL = np.expand_dims(np.squeeze(audio_genL.numpy()), axis=1)\n",
        "audio_genR = np.expand_dims(np.squeeze(audio_genR.numpy()), axis=1)\n",
        "audio_genS = np.concatenate((audio_genL, audio_genR), axis=1)\n",
        "audio_genM = np.expand_dims(np.squeeze(audio_genM.numpy()), axis=1)\n",
        "\n",
        "write_audio(\"renderS.wav\", 48000, audio_genS)\n",
        "write_audio(\"renderM.wav\", 48000, audio_genM)\n",
        "\n",
        "!ffmpeg-normalize renderS.wav -o normalized/render.wav -t -15 -ar 48000\n",
        "\n",
        "colab_utils.download(\"normalized/render.wav\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etgf80MPGWQz"
      },
      "source": [
        "## **Step 14** (optional)--Download your model for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbx-cZJOD4UR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4bb6cd27-ea2b-4779-a8cc-d78f92b3ad61"
      },
      "source": [
        "%cd $CKPT_OUTPUT_DIR\n",
        "!zip -r checkpoint.zip *\n",
        "colab_utils.download('checkpoint.zip')\n",
        "!rm checkpoint.zip"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}
