{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ddsp_48kHz_stereo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hMqWDc_m6rUC"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJd3dlOCStH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlexCouncil/DDSP-48kHz-Stereo/blob/master/ddsp/colab/ddsp_48kHz_stereo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMqWDc_m6rUC"
      },
      "source": [
        "\n",
        "##### Copyright 2020 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNhgka4UKNjf"
      },
      "source": [
        "# Copyright 2020 Google LLC. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpXo6phTiOQM"
      },
      "source": [
        "# Train & Timbre Transfer--DDSP Autoencoder on GPU--48 kHz/Stereo\n",
        "\n",
        "Made by [Google Magenta](https://magenta.tensorflow.org/)--altered by [Flex Council](https://soundcloud.com/flexcouncil)\n",
        "\n",
        "**A Little Background**\n",
        "\n",
        "A producer friend of mine turned me on to Magenta’s DDSP, and I’m glad he did. In my mind it represents the way forward for AI music. Finally we have a glimpse inside the black box, with access to musical parameters as well as neural net hyperparameters. And DDSP leverages decades of studio knowledge by utilizing traditional processors like synthesizers and effects. One can envision a time when DDSP-like elements will sit at the heart of production DAWs.\n",
        "\n",
        "DDSP will accept most audio sample rates and formats. However, native 48kHz/stereo datasets and primers will sound best. Output files are always 48kHz/stereo. You can upload datasets and primers via the browser or use Google Drive.\n",
        "\n",
        "The algorithm was designed to model single instruments played monophonically, but it can also produce interesting results with denser, polyphonic material and percussion.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/ddsp/additive_diagram/ddsp_autoencoder.png\" alt=\"DDSP Autoencoder figure\" width=\"700\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXjcauVRB48S"
      },
      "source": [
        "**Note that we prefix bash commands with a `!` inside of Colab, but you would leave them out if running directly in a terminal.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn7CQ4GQizHy"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        "First we install the required dependencies with `pip`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxPuPR0j5Gs7",
        "outputId": "062bdc9a-2c9e-44bd-88e3-ed55d440ef36"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "# !pip install -qU ddsp[data_preparation]\n",
        "!pip install -qU git+https://github.com/FlexCouncil/ddsp4@ddsp\n",
        "\n",
        "# Initialize global path for using google drive. \n",
        "DRIVE_DIR = ''\n",
        "\n",
        "# Helper Functions\n",
        "sample_rate = 48000\n",
        "n_fft = 6144"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0fVn8yUJl_v"
      },
      "source": [
        "### Setup Google Drive (Optional, Recommeded)\n",
        "\n",
        "This notebook requires uploading audio and saving checkpoints. While you can do this with direct uploads / downloads, it is recommended to connect to your google drive account. This will enable faster file transfer, and regular saving of checkpoints so that you do not lose your work if the colab kernel restarts (common for training more than 12 hours). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6MXUbL6KeMn"
      },
      "source": [
        "#### Login and mount your drive\n",
        "\n",
        "This will require an authentication code. You should then be able to see your drive in the file browser on the left panel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m33xuTjEKazJ",
        "outputId": "ba5831a9-085f-4bce-f6d6-afa8e273891d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4vmxpj1LC7m"
      },
      "source": [
        "#### Set your base directory\n",
        "* In drive, put all of the audio files with which you would like to train in a single folder.\n",
        " * Typically works well with 10-20 minutes of audio from a single monophonic source (also, one acoustic environment). \n",
        "* Use the file browser in the left panel to find a folder with your audio, right-click **\"Copy Path\", paste below**, and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0bK6P9DMBTb",
        "outputId": "94e67a6e-b32b-484b-9f1c-baeb4b31c222"
      },
      "source": [
        "#@markdown (ex. `/content/drive/My Drive/...`) Leave blank to skip loading from Drive.\n",
        "DRIVE_DIR = '' #@param {type: \"string\"}\n",
        "\n",
        "import os\n",
        "assert os.path.exists(DRIVE_DIR)\n",
        "print('Drive Folder Exists:', DRIVE_DIR)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FELlizMtIxCH"
      },
      "source": [
        "### Make directories to save model and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd22WxEQI3FV",
        "outputId": "44b96410-205b-446e-f487-51fdc09a5d8f"
      },
      "source": [
        "#@markdown Check the box below if you'd like to train with latent vectors.\n",
        "\n",
        "LATENT_VECTORS = False #@param{type:\"boolean\"}\n",
        "\n",
        "!git clone https://github.com/FlexCouncil/gin.git\n",
        "\n",
        "if LATENT_VECTORS:\n",
        "  GIN_FILE = 'gin/solo_instrument.gin'\n",
        "else:\n",
        "  GIN_FILE = 'gin/solo_instrument_noz.gin'\n",
        "\n",
        "AUDIO_DIR_LEFT = 'data/audio-left'\n",
        "AUDIO_DIR_RIGHT = 'data/audio-right'\n",
        "MODEL_DIR_LEFT = 'data/model-left'\n",
        "MODEL_DIR_RIGHT = 'data/model-right'\n",
        "AUDIO_FILEPATTERN_LEFT = AUDIO_DIR_LEFT + '/*'\n",
        "AUDIO_FILEPATTERN_RIGHT = AUDIO_DIR_RIGHT + '/*'\n",
        "!mkdir -p $AUDIO_DIR_LEFT $AUDIO_DIR_RIGHT $MODEL_DIR_LEFT $MODEL_DIR_RIGHT\n",
        "\n",
        "if DRIVE_DIR:\n",
        "  SAVE_DIR_LEFT = os.path.join(DRIVE_DIR, 'ddsp-solo-instrument-left')\n",
        "  SAVE_DIR_RIGHT = os.path.join(DRIVE_DIR, 'ddsp-solo-instrument-right')\n",
        "  INPUT_DIR = os.path.join(DRIVE_DIR, 'dataset-input')\n",
        "  PRIMERS_DIR = os.path.join(DRIVE_DIR, 'primers')\n",
        "  OUTPUT_DIR = os.path.join(DRIVE_DIR, 'resynthesis-output')\n",
        "\n",
        "  !mkdir -p \"$SAVE_DIR_LEFT\" \"$SAVE_DIR_RIGHT\" \"$INPUT_DIR\" \"$PRIMERS_DIR\" \"$OUTPUT_DIR\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNhH7nEbX2db"
      },
      "source": [
        "### Upload training audio\n",
        "\n",
        "Upload training audio to the \"dataset-input\" folder inside the DRIVE_DIR folder if using Drive (otherwise prompts local upload.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itVKEzF6m3rY",
        "outputId": "fd30c591-4154-4f82-fda2-b9d086827422"
      },
      "source": [
        "!pip install note_seq\n",
        "\n",
        "import glob\n",
        "import os\n",
        "from ddsp.colab import colab_utils\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "from scipy.io.wavfile import write as write_audio\n",
        "\n",
        "if DRIVE_DIR:\n",
        "  wav_files = glob.glob(os.path.join(INPUT_DIR, '*.wav'))\n",
        "  aiff_files = glob.glob(os.path.join(INPUT_DIR, '*.aiff'))\n",
        "  aif_files = glob.glob(os.path.join(INPUT_DIR, '*.aif'))\n",
        "  ogg_files = glob.glob(os.path.join(INPUT_DIR, '*.ogg'))\n",
        "  flac_files = glob.glob(os.path.join(INPUT_DIR, '*.flac'))\n",
        "  mp3_files = glob.glob(os.path.join(INPUT_DIR, '*.mp3'))\n",
        "  audio_files = wav_files + aiff_files + aif_files + ogg_files + flac_files + mp3_files\n",
        "else:\n",
        "  uploaded_files = files.upload()\n",
        "  audio_files = list(uploaded_files.keys())\n",
        "\n",
        "for fname in audio_files:\n",
        "  # Convert to 48kHz.\n",
        "  audio, unused_sample_rate = librosa.load(fname, sr=48000, mono=False)\n",
        "  if (audio.ndim == 2):\n",
        "    audio = np.swapaxes(audio, 0, 1)\n",
        "  # Mono to stereo.\n",
        "  if (audio.ndim == 1):\n",
        "    print('Converting mono to stereo.')\n",
        "    audio = np.stack((audio, audio), axis=-1)\n",
        "  target_name_left = os.path.join(AUDIO_DIR_LEFT, \n",
        "                             os.path.basename(fname).replace(' ', '_').replace('aiff', 'wav').replace('aif', 'wav').replace('ogg', 'wav').replace('flac', 'wav').replace('mp3', 'wav'))\n",
        "  target_name_right = os.path.join(AUDIO_DIR_RIGHT, \n",
        "                             os.path.basename(fname).replace(' ', '_').replace('aiff', 'wav').replace('aif', 'wav').replace('ogg', 'wav').replace('flac', 'wav').replace('mp3', 'wav'))\n",
        "  # Split to dual mono.\n",
        "  write_audio(target_name_left, sample_rate, audio[:, 0])\n",
        "  write_audio(target_name_right, sample_rate, audio[:, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_XVFoN2YOat"
      },
      "source": [
        "### Preprocess raw audio into TFRecord dataset\n",
        "\n",
        "We need to do some preprocessing on the raw audio you uploaded to get it into the correct format for training. This involves turning the full audio into short (4-second) examples, inferring the fundamental frequency (or \"pitch\") with [CREPE](http://github.com/marl/crepe), and computing the loudness. These features will then be stored in a sharded [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) file for easier loading. Depending on the amount of input audio, this process usually takes a few minutes.\n",
        "\n",
        "* (Optional) Transfer dataset from drive. If you've already created a dataset, from a previous run, this cell will skip the dataset creation step and copy the dataset from `$DRIVE_DIR/data` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MsnkAHyHVrCW",
        "outputId": "797cd847-99b2-442a-8f5d-852259d6d6ea"
      },
      "source": [
        "!pip install apache_beam\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "TRAIN_TFRECORD_LEFT = 'data/train-left.tfrecord'\n",
        "TRAIN_TFRECORD_RIGHT = 'data/train-right.tfrecord'\n",
        "TRAIN_TFRECORD_FILEPATTERN_LEFT = TRAIN_TFRECORD_LEFT + '*'\n",
        "TRAIN_TFRECORD_FILEPATTERN_RIGHT = TRAIN_TFRECORD_RIGHT + '*'\n",
        "\n",
        "# Copy dataset from drive if dataset has already been created.\n",
        "drive_data_dir = os.path.join(DRIVE_DIR, 'data') \n",
        "drive_dataset_files = glob.glob(drive_data_dir + '/*')\n",
        "\n",
        "if DRIVE_DIR and len(drive_dataset_files) > 0:\n",
        "  !cp \"$drive_data_dir\"/* data/\n",
        "\n",
        "else:\n",
        "  # Make a new dataset.\n",
        "  if (not glob.glob(AUDIO_FILEPATTERN_LEFT)) or (not glob.glob(AUDIO_FILEPATTERN_RIGHT)):\n",
        "    raise ValueError('No audio files found. Please use the previous cell to '\n",
        "                    'upload.')\n",
        "\n",
        "  !ddsp_prepare_tfrecord \\\n",
        "    --input_audio_filepatterns=$AUDIO_FILEPATTERN_LEFT \\\n",
        "    --output_tfrecord_path=$TRAIN_TFRECORD_LEFT \\\n",
        "    --num_shards=10 \\\n",
        "    --sample_rate=$sample_rate \\\n",
        "    --alsologtostderr\n",
        "\n",
        "  !ddsp_prepare_tfrecord \\\n",
        "    --input_audio_filepatterns=$AUDIO_FILEPATTERN_RIGHT \\\n",
        "    --output_tfrecord_path=$TRAIN_TFRECORD_RIGHT \\\n",
        "    --num_shards=10 \\\n",
        "    --sample_rate=$sample_rate \\\n",
        "    --alsologtostderr\n",
        "\n",
        "  # Copy dataset to drive for safe-keeping.\n",
        "  if DRIVE_DIR:\n",
        "    !mkdir \"$drive_data_dir\"/\n",
        "    print('Saving to {}'.format(drive_data_dir))\n",
        "    !cp $TRAIN_TFRECORD_FILEPATTERN_LEFT \"$drive_data_dir\"/\n",
        "    !cp $TRAIN_TFRECORD_FILEPATTERN_RIGHT \"$drive_data_dir\"/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dill",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4toX-D-AYZL"
      },
      "source": [
        "### Save dataset statistics for timbre transfer\n",
        "\n",
        "Quantile normalization helps match loudness of timbre transfer inputs to the loudness of the dataset, so let's calculate it here and save in a pickle file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp_c8P0xApY6",
        "outputId": "ea79aad4-443a-4747-b7e7-976d0e25c801"
      },
      "source": [
        "from ddsp.colab import colab_utils\n",
        "import ddsp.training\n",
        "\n",
        "data_provider_left = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN_LEFT, sample_rate=sample_rate)\n",
        "data_provider_right = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN_RIGHT, sample_rate=sample_rate)\n",
        "dataset_left = data_provider_left.get_dataset(shuffle=False)\n",
        "dataset_right = data_provider_right.get_dataset(shuffle=False)\n",
        "\n",
        "if DRIVE_DIR:\n",
        "  PICKLE_FILE_PATH_LEFT = os.path.join(SAVE_DIR_LEFT, 'dataset_statistics_left.pkl')\n",
        "  PICKLE_FILE_PATH_RIGHT = os.path.join(SAVE_DIR_RIGHT, 'dataset_statistics_right.pkl')\n",
        "else:\n",
        "  PICKLE_FILE_PATH_LEFT = os.path.join(MODEL_DIR_LEFT, 'dataset_statistics_left.pkl')\n",
        "  PICKLE_FILE_PATH_RIGHT = os.path.join(MODEL_DIR_RIGHT, 'dataset_statistics_right.pkl')\n",
        "\n",
        "colab_utils.save_dataset_statistics(data_provider_left, PICKLE_FILE_PATH_LEFT, batch_size=1)\n",
        "colab_utils.save_dataset_statistics(data_provider_right, PICKLE_FILE_PATH_RIGHT, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIsq0HrzbOF7"
      },
      "source": [
        "Let's load the dataset in the `ddsp` library and have a look at one of the examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dA-FOmRgYdpZ",
        "outputId": "aa92a77a-8012-4081-cbc2-4aca3877cc43"
      },
      "source": [
        "from ddsp.colab import colab_utils\n",
        "import ddsp.training\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data_provider_left = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN_LEFT, sample_rate=sample_rate)\n",
        "dataset_left = data_provider_left.get_dataset(shuffle=False)\n",
        "\n",
        "data_provider_right = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN_RIGHT, sample_rate=sample_rate)\n",
        "dataset_right = data_provider_right.get_dataset(shuffle=False)\n",
        "\n",
        "try:\n",
        "  ex_left = next(iter(dataset_left))\n",
        "except StopIteration:\n",
        "  raise ValueError(\n",
        "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
        "      'different audio file(s).')\n",
        "\n",
        "try:\n",
        "  ex_right = next(iter(dataset_right))\n",
        "except StopIteration:\n",
        "  raise ValueError(\n",
        "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
        "      'different audio file(s).')\n",
        "\n",
        "print('Top: Left, Bottom: Right')\n",
        "colab_utils.specplot(ex_left['audio'])\n",
        "colab_utils.specplot(ex_right['audio'])\n",
        "\n",
        "f, ax = plt.subplots(6, 1, figsize=(14, 12))\n",
        "x = np.linspace(0, 4.0, 1000)\n",
        "ax[0].set_ylabel('loudness_db L')\n",
        "ax[0].plot(x, ex_left['loudness_db'])\n",
        "ax[1].set_ylabel('loudness_db R')\n",
        "ax[1].plot(x, ex_right['loudness_db'])\n",
        "ax[2].set_ylabel('F0_Hz L')\n",
        "ax[2].set_xlabel('seconds')\n",
        "ax[2].plot(x, ex_left['f0_hz'])\n",
        "ax[3].set_ylabel('F0_Hz R')\n",
        "ax[3].set_xlabel('seconds')\n",
        "ax[3].plot(x, ex_right['f0_hz'])\n",
        "ax[4].set_ylabel('F0_confidence L')\n",
        "ax[4].set_xlabel('seconds')\n",
        "ax[4].plot(x, ex_left['f0_confidence'])\n",
        "ax[5].set_ylabel('F0_confidence R')\n",
        "ax[5].set_xlabel('seconds')\n",
        "ax[5].plot(x, ex_right['f0_confidence'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0250103fd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gvXBa7PbuyY"
      },
      "source": [
        "### Train Model\n",
        "\n",
        "We will now train a \"solo instrument\" model. This means the model is conditioned only on the fundamental frequency (f0) and loudness with no instrument ID or latent timbre feature. If you uploaded audio of multiple instruemnts, the neural network you train will attempt to model all timbres, but will likely associate certain timbres with different f0 and loudness conditions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpwQkSIKjEMZ"
      },
      "source": [
        "First, let's start up a [TensorBoard](https://www.tensorflow.org/tensorboard) to monitor our loss as training proceeds. \n",
        "\n",
        "Initially, TensorBoard will report `No dashboards are active for the current data set.`, but once training begins, the dashboards should appear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCaI9PNLvXbf"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "import tensorboard as tb\n",
        "if DRIVE_DIR:\n",
        "  tb.notebook.start('--logdir \"{}\"'.format(SAVE_DIR_LEFT))\n",
        "  tb.notebook.start('--logdir \"{}\"'.format(SAVE_DIR_RIGHT))\n",
        "else:\n",
        "  tb.notebook.start('--logdir \"{}\"'.format(MODEL_DIR_LEFT))\n",
        "  tb.notebook.start('--logdir \"{}\"'.format(MODEL_DIR_RIGHT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT-8Koyvj46w"
      },
      "source": [
        "### We will now begin training. \n",
        "\n",
        "Note that we specify [gin configuration](https://github.com/google/gin-config) files for the both the model architecture ([solo_instrument.gin](TODO)) and the dataset ([tfrecord.gin](TODO)), which are both predefined in the library. You could also create your own. We then override some of the spefic params for `batch_size` (which is defined in in the model gin file) and the tfrecord path (which is defined in the dataset file). \n",
        "\n",
        "#### Training Notes:\n",
        "* Models typically perform well when the loss drops to the range of ~5.0-7.0.\n",
        "* Depending on the dataset this can take anywhere from 5k-40k training steps usually.\n",
        "* On the colab GPU, this can take from around 3-24 hours. \n",
        "* We **highly recommend** saving checkpoints directly to your drive account as colab will restart naturally after about 12 hours and you may lose all of your checkpoints.\n",
        "* By default, checkpoints will be saved every 250 steps with a maximum of 10 checkpoints (at ~60MB/checkpoint this is ~600MB). Feel free to adjust these numbers depending on the frequency of saves you would like and space on your drive.\n",
        "* If you're restarting a session and `DRIVE_DIR` points a directory that was previously used for training, training should resume at the last checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poKO-mZEGYXZ",
        "outputId": "ca9c4192-0057-45ea-c513-f27df9b95a21"
      },
      "source": [
        "#@markdown Enter number of steps to train. Restart runtime to interrupt training.\n",
        "\n",
        "NUM_STEPS = 1000 #@param {type:\"slider\", min: 1000, max:40000, step:1000}\n",
        "NUM_LOOPS = int(NUM_STEPS / 1000)\n",
        "\n",
        "if DRIVE_DIR:\n",
        "  TRAIN_DIR_LEFT = SAVE_DIR_LEFT\n",
        "  TRAIN_DIR_RIGHT = SAVE_DIR_RIGHT\n",
        "else:\n",
        "  TRAIN_DIR_LEFT = MODEL_DIR_LEFT\n",
        "  TRAIN_DIR_RIGHT = MODEL_DIR_RIGHT\n",
        "\n",
        "for i in range (0, NUM_LOOPS):\n",
        "  !ddsp_run \\\n",
        "    --mode=train \\\n",
        "    --alsologtostderr \\\n",
        "    --save_dir=\"$TRAIN_DIR_LEFT\" \\\n",
        "    --gin_file=\"$GIN_FILE\" \\\n",
        "    --gin_file=\"gin/datasets/tfrecord.gin\" \\\n",
        "    --gin_param=\"TFRecordProvider.file_pattern='$TRAIN_TFRECORD_FILEPATTERN_LEFT'\" \\\n",
        "    --gin_param=\"batch_size=6\" \\\n",
        "    --gin_param=\"train_util.train.num_steps=1000\" \\\n",
        "    --gin_param=\"train_util.train.steps_per_save=250\" \\\n",
        "    --gin_param=\"trainers.Trainer.checkpoints_to_keep=10\"\n",
        "\n",
        "  !ddsp_run \\\n",
        "    --mode=train \\\n",
        "    --alsologtostderr \\\n",
        "    --save_dir=\"$TRAIN_DIR_RIGHT\" \\\n",
        "    --gin_file=\"$GIN_FILE\" \\\n",
        "    --gin_file=\"gin/datasets/tfrecord.gin\" \\\n",    
        "    --gin_param=\"TFRecordProvider.file_pattern='$TRAIN_TFRECORD_FILEPATTERN_RIGHT'\" \\\n",
        "    --gin_param=\"batch_size=6\" \\\n",
        "    --gin_param=\"train_util.train.num_steps=1000\" \\\n",
        "    --gin_param=\"train_util.train.steps_per_save=250\" \\\n",
        "    --gin_param=\"trainers.Trainer.checkpoints_to_keep=10\"\n",
        "\n",
        "  # Remove extra gin files.\n",
        "  if DRIVE_DIR:\n",
        "    !cd \"$SAVE_DIR_LEFT\" && mv \"operative_config-0.gin\" \"$DRIVE_DIR\"\n",
        "    !cd \"$SAVE_DIR_LEFT\" && rm operative_config*\n",
        "    !cd \"$DRIVE_DIR\" && mv \"operative_config-0.gin\" \"$SAVE_DIR_LEFT\"\n",
        "    !cd \"$SAVE_DIR_RIGHT\" && mv \"operative_config-0.gin\" \"$DRIVE_DIR\"\n",
        "    !cd \"$SAVE_DIR_RIGHT\" && rm operative_config*\n",
        "    !cd \"$DRIVE_DIR\" && mv \"operative_config-0.gin\" \"$SAVE_DIR_RIGHT\"\n",
        "  else:\n",
        "    !cd \"$MODEL_DIR_LEFT\" && mv \"operative_config-0.gin\" \"$AUDIO_DIR_LEFT\"\n",
        "    !cd \"$MODEL_DIR_LEFT\" && rm operative_config*\n",
        "    !cd \"$AUDIO_DIR_LEFT\" && mv \"operative_config-0.gin\" \"$MODEL_DIR_LEFT\"\n",
        "    !cd \"$MODEL_DIR_RIGHT\" && mv \"operative_config-0.gin\" \"$AUDIO_DIR_RIGHT\"\n",
        "    !cd \"$MODEL_DIR_RIGHT\" && rm operative_config*\n",
        "    !cd \"$AUDIO_DIR_RIGHT\" && mv \"operative_config-0.gin\" \"$MODEL_DIR_RIGHT\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V95qxVjFzWR6"
      },
      "source": [
        "### Resynthesis\n",
        "\n",
        "Check how well the model reconstructs the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OQ5PPDZVzgFR",
        "outputId": "f81dc96f-510c-4581-ea13-cbf186a0327d"
      },
      "source": [
        "!pip install note_seq\n",
        "\n",
        "from ddsp.colab.colab_utils import play, specplot, download\n",
        "import ddsp.training\n",
        "import gin\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from scipy.io.wavfile import write as write_audio\n",
        "\n",
        "data_provider_left = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN_LEFT, sample_rate=sample_rate)\n",
        "data_provider_right = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN_RIGHT, sample_rate=sample_rate)\n",
        "dataset_left = data_provider_left.get_batch(batch_size=1, shuffle=False)\n",
        "dataset_right = data_provider_right.get_batch(batch_size=1, shuffle=False)\n",
        "\n",
        "try:\n",
        "  batch_left = next(iter(dataset_left))\n",
        "except OutOfRangeError:\n",
        "  raise ValueError(\n",
        "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
        "      'different audio file(s).')\n",
        "  \n",
        "try:\n",
        "  batch_right = next(iter(dataset_right))\n",
        "except OutOfRangeError:\n",
        "  raise ValueError(\n",
        "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
        "      'different audio file(s).')\n",
        "\n",
        "# Parse the gin configs.\n",
        "if DRIVE_DIR:\n",
        "  gin_file_left = os.path.join(SAVE_DIR_LEFT, 'operative_config-0.gin')\n",
        "  gin_file_right = os.path.join(SAVE_DIR_RIGHT, 'operative_config-0.gin')\n",
        "else:\n",
        "  gin_file_left = os.path.join(MODEL_DIR_LEFT, 'operative_config-0.gin')\n",
        "  gin_file_right = os.path.join(MODEL_DIR_RIGHT, 'operative_config-0.gin')\n",
        "\n",
        "gin.parse_config_file(gin_file_left)\n",
        "gin.parse_config_file(gin_file_right)\n",
        "\n",
        "# Load models\n",
        "model_left = ddsp.training.models.Autoencoder()\n",
        "model_right = ddsp.training.models.Autoencoder()\n",
        "\n",
        "if DRIVE_DIR:\n",
        "  model_left.restore(SAVE_DIR_LEFT)\n",
        "  model_right.restore(SAVE_DIR_RIGHT)\n",
        "else:\n",
        "  model_left.restore(MODEL_DIR_LEFT)\n",
        "  model_right.restore(MODEL_DIR_RIGHT)\n",
        "\n",
        "# Resynthesize audio.\n",
        "audio_left = batch_left['audio']\n",
        "audio_right = batch_right['audio']\n",
        "\n",
        "outputs_left = model_left(batch_left, training=False)\n",
        "audio_gen_left = model_left.get_audio_from_outputs(outputs_left)\n",
        "outputs_right = model_right(batch_right, training=False)\n",
        "audio_gen_right = model_right.get_audio_from_outputs(outputs_right)\n",
        "\n",
        "# Merge to stereo.\n",
        "audio_left_stereo = np.expand_dims(np.squeeze(audio_left.numpy()), axis=1)\n",
        "audio_right_stereo = np.expand_dims(np.squeeze(audio_right.numpy()), axis=1)\n",
        "audio_stereo = np.concatenate((audio_left_stereo, audio_right_stereo), axis=1)\n",
        "\n",
        "audio_gen_left_stereo = np.expand_dims(np.squeeze(audio_gen_left.numpy()), axis=1)\n",
        "audio_gen_right_stereo = np.expand_dims(np.squeeze(audio_gen_right.numpy()), axis=1)\n",
        "audio_gen_stereo = np.concatenate((audio_gen_left_stereo, audio_gen_right_stereo), axis=1)\n",
        "\n",
        "# Play.\n",
        "print('Original Audio')\n",
        "play(audio_stereo, sample_rate=sample_rate)\n",
        "\n",
        "print('Resynthesis')\n",
        "play(audio_gen_stereo, sample_rate=sample_rate)\n",
        "\n",
        "# Plot.\n",
        "print('Spectrograms: Top two are Original Audio L/R, bottom two are Resynthesis L/R')\n",
        "specplot(audio_left)\n",
        "specplot(audio_right)\n",
        "specplot(audio_gen_left)\n",
        "specplot(audio_gen_right)\n",
        "\n",
        "WRITE_PATH = OUTPUT_DIR + \"/resynthesis.wav\"\n",
        "\n",
        "write_audio(\"resynthesis.wav\", sample_rate, audio_gen_stereo)\n",
        "write_audio(WRITE_PATH, sample_rate, audio_gen_stereo)\n",
        "\n",
        "!ffmpeg-normalize resynthesis.wav -o resynthesis.wav -t -15 -ar 48000 -f\n",
        "\n",
        "download(\"resynthesis.wav\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id=\"id_1\"> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id=\"id_2\"> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a83e677b-b2d2-4f57-a9db-08cc8ee2d017\", \"resynthesis.wav\", 1536058)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SENH2Zvg81kd"
      },
      "source": [
        "## Timbre Transfer\n",
        "\n",
        "### Install & Import\n",
        "\n",
        "Install ddsp, define some helper functions, and download the model. This transfers a lot of data and should take a minute or two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NzIY3_ayABb",
        "outputId": "53035188-8be8-4af7-f285-a771892d53d7"
      },
      "source": [
        "# Ignore a bunch of deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "\n",
        "import crepe\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "from ddsp.colab import colab_utils\n",
        "from ddsp.colab.colab_utils import (\n",
        "    auto_tune, detect_notes, fit_quantile_transform, \n",
        "    get_tuning_factor, download, play, record, \n",
        "    specplot, upload, DEFAULT_SAMPLE_RATE)\n",
        "import gin\n",
        "from google.colab import files\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Helper Functions\n",
        "sample_rate = 48000\n",
        "n_fft = 2048\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqCdWxNpejBP"
      },
      "source": [
        "### Primer Audio File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ILIWQdXAzg6R",
        "outputId": "d3308bf5-7f4e-4cfe-fbfc-25a7df280229"
      },
      "source": [
        "from google.colab import files\n",
        "from ddsp.colab.colab_utils import play\n",
        "\n",
        "import re\n",
        "\n",
        "#@markdown * Audio should be monophonic (single instrument / voice).\n",
        "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
        "#@markdown * Choose an audio file on Drive or upload an audio file.\n",
        "#@markdown * If you are using Drive, place the audio file in the \"primers\" folder inside the DRIVE_DIR folder. Enter the file name below.\n",
        "\n",
        "PRIMER_FILE =  \"\" #@param {type:\"string\"}\n",
        "\n",
        "DRIVE_OR_UPLOAD = \"Drive\"  #@param [\"Drive\", \"Upload (.wav)\"]\n",
        "\n",
        "# Check for .wav extension.\n",
        "match = re.search(r'.wav', PRIMER_FILE)\n",
        "if match:\n",
        "  print ('')\n",
        "else:\n",
        "  PRIMER_FILE = PRIMER_FILE + \".wav\"\n",
        "\n",
        "if DRIVE_OR_UPLOAD == \"Drive\":\n",
        "  PRIMER_PATH = PRIMERS_DIR + \"/\" + PRIMER_FILE\n",
        "  # Convert to 48kHz.\n",
        "  audio, unused_sample_rate = librosa.load(PRIMER_PATH, sr=48000, mono=False)\n",
        "  if (audio.ndim == 2):\n",
        "    audio = np.swapaxes(audio, 0, 1)\n",
        "else:\n",
        "  # Load audio sample here (.wav file)\n",
        "  # Just use the first file.\n",
        "  audio_files = files.upload()\n",
        "  fnames = list(audio_files.keys())\n",
        "  audios = []\n",
        "  for fname in fnames:\n",
        "    audio, unused_sample_rate = librosa.load(fname, sr=48000, mono=False)\n",
        "  if (audio.ndim == 2):\n",
        "    audio = np.swapaxes(audio, 0, 1)\n",
        "    audios.append(audio)\n",
        "  audio = audios[0]\n",
        "\n",
        "# Mono to stereo.\n",
        "if (audio.ndim == 1):\n",
        "  print('Converting mono to stereo.')\n",
        "  audio = np.stack((audio, audio), axis=-1)\n",
        "\n",
        "# Setup the session.\n",
        "ddsp.spectral_ops.reset_crepe()\n",
        "\n",
        "# Compute features.\n",
        "audio_left = np.squeeze(audio[:, 0]).astype(np.float32)\n",
        "audio_right = np.squeeze(audio[:, 1]).astype(np.float32)\n",
        "audio_left = audio_left[np.newaxis, :]\n",
        "audio_right = audio_right[np.newaxis, :]\n",
        "\n",
        "start_time = time.time()\n",
        "audio_features_left = ddsp.training.metrics.compute_audio_features(audio_left, n_fft=n_fft, sample_rate=sample_rate)\n",
        "audio_features_right = ddsp.training.metrics.compute_audio_features(audio_right, n_fft=n_fft, sample_rate=sample_rate)\n",
        "\n",
        "audio_features_left['loudness_db'] = audio_features_left['loudness_db'].astype(np.float32)\n",
        "audio_features_right['loudness_db'] = audio_features_right['loudness_db'].astype(np.float32)\n",
        "audio_features_mod_left = None\n",
        "audio_features_mod_right = None\n",
        "print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "play(audio, sample_rate=sample_rate)\n",
        "\n",
        "TRIM = -15\n",
        "# Plot Features.\n",
        "fig, ax = plt.subplots(nrows=6, \n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(6, 16))\n",
        "ax[0].plot(audio_features_left['loudness_db'][:TRIM])\n",
        "ax[0].set_ylabel('loudness_db L')\n",
        "\n",
        "ax[1].plot(audio_features_right['loudness_db'][:TRIM])\n",
        "ax[1].set_ylabel('loudness_db R')\n",
        "\n",
        "ax[2].plot(librosa.hz_to_midi(audio_features_left['f0_hz'][:TRIM]))\n",
        "ax[2].set_ylabel('f0 [midi] L')\n",
        "\n",
        "ax[3].plot(librosa.hz_to_midi(audio_features_right['f0_hz'][:TRIM]))\n",
        "ax[3].set_ylabel('f0 [midi] R')\n",
        "\n",
        "ax[4].plot(audio_features_left['f0_confidence'][:TRIM])\n",
        "ax[4].set_ylabel('f0 confidence L')\n",
        "_ = ax[4].set_xlabel('Time step [frame] L')\n",
        "\n",
        "ax[5].plot(audio_features_right['f0_confidence'][:TRIM])\n",
        "ax[5].set_ylabel('f0 confidence R')\n",
        "_ = ax[5].set_xlabel('Time step [frame] R')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id=\"id_13\"> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }        
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEyoIHrKzy2c"
      },
      "source": [
        "## Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhoIsgDiz1ak",
        "outputId": "4b40888f-7e1c-437e-a6bf-6e3274716228"
      },
      "source": [
        "def find_model_dir(dir_name):\n",
        "  # Iterate through directories until model directory is found\n",
        "  for root, dirs, filenames in os.walk(dir_name):\n",
        "    for filename in filenames:\n",
        "      if filename.endswith(\".gin\") and not filename.startswith(\".\"):\n",
        "        model_dir = root\n",
        "        break\n",
        "  return model_dir \n",
        "\n",
        "if DRIVE_DIR:\n",
        "  model_dir_left = find_model_dir(SAVE_DIR_LEFT)\n",
        "  model_dir_right = find_model_dir(SAVE_DIR_RIGHT)\n",
        "else:\n",
        "  model_dir_left = find_model_dir(MODEL_DIR_LEFT)\n",
        "  model_dir_right = find_model_dir(MODEL_DIR_RIGHT)\n",
        "\n",
        "gin_file_left = os.path.join(model_dir_left, 'operative_config-0.gin')\n",
        "gin_file_right = os.path.join(model_dir_right, 'operative_config-0.gin')\n",
        "\n",
        "# Load the dataset statistics.\n",
        "DATASET_STATS_LEFT = None\n",
        "DATASET_STATS_RIGHT = None\n",
        "dataset_stats_file_left = os.path.join(model_dir_left, 'dataset_statistics_left.pkl')\n",
        "dataset_stats_file_right = os.path.join(model_dir_right, 'dataset_statistics_right.pkl')\n",
        "\n",
        "print(f'Loading dataset statistics from {dataset_stats_file_left}')\n",
        "try:\n",
        "  if tf.io.gfile.exists(dataset_stats_file_left):\n",
        "    with tf.io.gfile.GFile(dataset_stats_file_left, 'rb') as f:\n",
        "      DATASET_STATS_LEFT = pickle.load(f)\n",
        "except Exception as err:\n",
        "  print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
        "\n",
        "print(f'Loading dataset statistics from {dataset_stats_file_right}')\n",
        "try:\n",
        "  if tf.io.gfile.exists(dataset_stats_file_right):\n",
        "    with tf.io.gfile.GFile(dataset_stats_file_right, 'rb') as f:\n",
        "      DATASET_STATS_RIGHT = pickle.load(f)\n",
        "except Exception as err:\n",
        "  print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
        "\n",
        "# Parse gin config,\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config_file(gin_file_left, skip_unknown=True)\n",
        "\n",
        "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
        "\n",
        "if DRIVE_DIR:\n",
        "  latest_checkpoint_fname_left = os.path.basename(tf.train.latest_checkpoint(SAVE_DIR_LEFT))\n",
        "  latest_checkpoint_fname_right = os.path.basename(tf.train.latest_checkpoint(SAVE_DIR_RIGHT))\n",
        "else:\n",
        "  latest_checkpoint_fname_left = os.path.basename(tf.train.latest_checkpoint(MODEL_DIR_LEFT))\n",
        "  latest_checkpoint_fname_right = os.path.basename(tf.train.latest_checkpoint(MODEL_DIR_RIGHT)) \n",
        "\n",
        "ckpt_left = os.path.join(model_dir_left, latest_checkpoint_fname_left)\n",
        "ckpt_right = os.path.join(model_dir_right, latest_checkpoint_fname_right)\n",
        "\n",
        "# Ensure dimensions and sampling rates are equal\n",
        "time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
        "n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "time_steps = int(audio_left.shape[1] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "# print(\"===Trained model===\")\n",
        "# print(\"Time Steps\", time_steps_train)\n",
        "# print(\"Samples\", n_samples_train)\n",
        "# print(\"Hop Size\", hop_size)\n",
        "# print(\"\\n===Resynthesis===\")\n",
        "# print(\"Time Steps\", time_steps)\n",
        "# print(\"Samples\", n_samples)\n",
        "# print('')\n",
        "\n",
        "gin_params = [\n",
        "    'Harmonic.n_samples = {}'.format(n_samples),\n",
        "    'FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "    'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),\n",
        "    'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.\n",
        "]\n",
        "\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config(gin_params)\n",
        "\n",
        "# Trim all input vectors to correct lengths \n",
        "for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "  audio_features_left[key] = audio_features_left[key][:time_steps]\n",
        "  audio_features_right[key] = audio_features_right[key][:time_steps]\n",
        "audio_features_left['audio'] = audio_features_left['audio'][:, :n_samples]\n",
        "audio_features_right['audio'] = audio_features_right['audio'][:, :n_samples]\n",
        "\n",
        "# Set up the model just to predict audio given new conditioning\n",
        "model_left = ddsp.training.models.Autoencoder()\n",
        "model_right = ddsp.training.models.Autoencoder()\n",
        "model_left.restore(ckpt_left)\n",
        "model_right.restore(ckpt_right)\n",
        "\n",
        "# Build model by running a batch through it.\n",
        "start_time = time.time()\n",
        "unused_left = model_left(audio_features_left, training=False)\n",
        "unused_right = model_right(audio_features_right, training=False)\n",
        "print('Restoring model took %.1f seconds' % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "VnPle9y0z9fd",
        "outputId": "6957c45e-c079-4f08-c227-3a6db437c23b"
      },
      "source": [
        "#@title Modify conditioning\n",
        "\n",
        "#@markdown These models were not explicitly trained to perform timbre transfer, so they may sound unnatural if the incoming loudness and frequencies are very different then the training data (which will always be somewhat true). \n",
        "\n",
        "\n",
        "#@markdown ## Note Detection\n",
        "\n",
        "#@markdown You can leave this at 1.0 for most cases\n",
        "threshold = 1 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
        "\n",
        "\n",
        "#@markdown ## Automatic\n",
        "\n",
        "ADJUST = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown Quiet parts without notes detected (dB)\n",
        "quiet = 30 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
        "\n",
        "#@markdown Force pitch to nearest note (amount)\n",
        "autotune = 0 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown ## Manual\n",
        "\n",
        "\n",
        "#@markdown Shift the pitch (octaves)\n",
        "pitch_shift =  0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "\n",
        "#@markdown Adjsut the overall loudness (dB)\n",
        "loudness_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "\n",
        "\n",
        "audio_features_mod_left = {k: v.copy() for k, v in audio_features_left.items()}\n",
        "audio_features_mod_right = {k: v.copy() for k, v in audio_features_right.items()}\n",
        "\n",
        "\n",
        "## Helper functions.\n",
        "def shift_ld(audio_features, ld_shift=0.0):\n",
        "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
        "  audio_features['loudness_db'] += ld_shift\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, pitch_shift=0.0):\n",
        "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
        "  audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n",
        "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
        "                                    0.0, \n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "mask_on_left = None\n",
        "mask_on_right = None\n",
        "\n",
        "\n",
        "if ADJUST and DATASET_STATS_LEFT and DATASET_STATS_RIGHT is not None:\n",
        "  # Detect sections that are \"on\".\n",
        "  mask_on_left, note_on_value_left = detect_notes(audio_features_left['loudness_db'],\n",
        "                                        audio_features_left['f0_confidence'],\n",
        "                                        threshold)\n",
        "  \n",
        "  mask_on_right, note_on_value_right = detect_notes(audio_features_right['loudness_db'],\n",
        "                                        audio_features_right['f0_confidence'],\n",
        "                                        threshold)\n",
        "\n",
        "  if np.any(mask_on_left) or np.any(mask_on_right):\n",
        "    # Shift the pitch register.\n",
        "    target_mean_pitch_left = DATASET_STATS_LEFT['mean_pitch']\n",
        "    target_mean_pitch_right = DATASET_STATS_RIGHT['mean_pitch']\n",
        "    pitch_left = ddsp.core.hz_to_midi(audio_features_left['f0_hz'])\n",
        "    pitch_right = ddsp.core.hz_to_midi(audio_features_right['f0_hz'])\n",
        "    mean_pitch_left = np.mean(pitch_left[mask_on_left])\n",
        "    mean_pitch_right = np.mean(pitch_right[mask_on_right])\n",
        "    p_diff_left = target_mean_pitch_left - mean_pitch_left\n",
        "    p_diff_right = target_mean_pitch_right - mean_pitch_right\n",
        "    p_diff_octave_left = p_diff_left / 12.0\n",
        "    p_diff_octave_right = p_diff_right / 12.0\n",
        "    round_fn_left = np.floor if p_diff_octave_left > 1.5 else np.ceil\n",
        "    round_fn_right = np.floor if p_diff_octave_right > 1.5 else np.ceil\n",
        "    p_diff_octave_left = round_fn_left(p_diff_octave_left)\n",
        "    p_diff_octave_right = round_fn_right(p_diff_octave_right)\n",
        "    audio_features_mod_left = shift_f0(audio_features_mod_left, p_diff_octave_left)\n",
        "    audio_features_mod_right = shift_f0(audio_features_mod_right, p_diff_octave_right)\n",
        "\n",
        "    # Quantile shift the note_on parts.\n",
        "    _, loudness_norm_left = colab_utils.fit_quantile_transform(\n",
        "        audio_features_left['loudness_db'],\n",
        "        mask_on_left,\n",
        "        inv_quantile=DATASET_STATS_LEFT['quantile_transform'])\n",
        "    \n",
        "    _, loudness_norm_right = colab_utils.fit_quantile_transform(\n",
        "        audio_features_right['loudness_db'],\n",
        "        mask_on_right,\n",
        "        inv_quantile=DATASET_STATS_RIGHT['quantile_transform'])\n",
        "\n",
        "    # Turn down the note_off parts.\n",
        "    mask_off_left = np.logical_not(mask_on_left)\n",
        "    mask_off_right = np.logical_not(mask_on_right)\n",
        "    loudness_norm_left[mask_off_left] -=  quiet * (1.0 - note_on_value_left[mask_off_left][:, np.newaxis])\n",
        "    loudness_norm_right[mask_off_right] -=  quiet * (1.0 - note_on_value_right[mask_off_right][:, np.newaxis])\n",
        "    loudness_norm_left = np.reshape(loudness_norm_left, audio_features_left['loudness_db'].shape)\n",
        "    loudness_norm_right = np.reshape(loudness_norm_right, audio_features_right['loudness_db'].shape)\n",
        "    \n",
        "    audio_features_mod_left['loudness_db'] = loudness_norm_left\n",
        "    audio_features_mod_right['loudness_db'] = loudness_norm_right \n",
        "\n",
        "    # Auto-tune.\n",
        "    if autotune:\n",
        "      f0_midi_left = np.array(ddsp.core.hz_to_midi(audio_features_mod_left['f0_hz']))\n",
        "      f0_midi_right = np.array(ddsp.core.hz_to_midi(audio_features_mod_right['f0_hz']))\n",
        "      tuning_factor_left = get_tuning_factor(f0_midi_left, audio_features_mod_left['f0_confidence'], mask_on_left)\n",
        "      tuning_factor_right = get_tuning_factor(f0_midi_right, audio_features_mod_right['f0_confidence'], mask_on_right)\n",
        "      f0_midi_at_left = auto_tune(f0_midi_left, tuning_factor_left, mask_on_left, amount=autotune)\n",
        "      f0_midi_at_right = auto_tune(f0_midi_right, tuning_factor_right, mask_on_right, amount=autotune)\n",
        "      audio_features_mod_left['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at_left)\n",
        "      audio_features_mod_right['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at_right)\n",
        "\n",
        "  else:\n",
        "    print('\\nSkipping auto-adjust (no notes detected or ADJUST box empty).')\n",
        "\n",
        "else:\n",
        "  print('\\nSkipping auto-adujst (box not checked or no dataset statistics found).')\n",
        "\n",
        "# Manual Shifts.\n",
        "audio_features_mod_left = shift_ld(audio_features_mod_left, loudness_shift)\n",
        "audio_features_mod_right = shift_ld(audio_features_mod_right, loudness_shift)\n",
        "audio_features_mod_left = shift_f0(audio_features_mod_left, pitch_shift)\n",
        "audio_features_mod_right = shift_f0(audio_features_mod_right, pitch_shift)\n",
        "\n",
        "# Plot Features.\n",
        "has_mask_left = int(mask_on_left is not None)\n",
        "has_mask_right = int(mask_on_right is not None)\n",
        "n_plots = 4 + has_mask_left + has_mask_right\n",
        "fig, axes = plt.subplots(nrows=n_plots, \n",
        "                      ncols=1, \n",
        "                      sharex=True,\n",
        "                      figsize=(2*n_plots, 10))\n",
        "\n",
        "if has_mask_left:\n",
        "  ax = axes[0]\n",
        "  ax.plot(np.ones_like(mask_on_left[:TRIM]) * threshold, 'k:')\n",
        "  ax.plot(note_on_value_left[:TRIM])\n",
        "  ax.plot(mask_on_left[:TRIM])\n",
        "  ax.set_ylabel('Note-on Mask L')\n",
        "  ax.set_xlabel('Time step [frame]')\n",
        "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
        "\n",
        "if has_mask_right:\n",
        "  ax = axes[0 + has_mask_left]\n",
        "  ax.plot(np.ones_like(mask_on_right[:TRIM]) * threshold, 'k:')\n",
        "  ax.plot(note_on_value_right[:TRIM])\n",
        "  ax.plot(mask_on_right[:TRIM])\n",
        "  ax.set_ylabel('Note-on Mask R')\n",
        "  ax.set_xlabel('Time step [frame]')\n",
        "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
        "\n",
        "ax = axes[0 + has_mask_left + has_mask_right]\n",
        "ax.plot(audio_features_left['loudness_db'][:TRIM])\n",
        "ax.plot(audio_features_mod_left['loudness_db'][:TRIM])\n",
        "ax.set_ylabel('loudness_db L')\n",
        "ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axes[1 + has_mask_left + has_mask_right]\n",
        "ax.plot(audio_features_right['loudness_db'][:TRIM])\n",
        "ax.plot(audio_features_mod_right['loudness_db'][:TRIM])\n",
        "ax.set_ylabel('loudness_db R')\n",
        "ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axes[2 + has_mask_left + has_mask_right]\n",
        "ax.plot(librosa.hz_to_midi(audio_features_left['f0_hz'][:TRIM]))\n",
        "ax.plot(librosa.hz_to_midi(audio_features_mod_left['f0_hz'][:TRIM]))\n",
        "ax.set_ylabel('f0 [midi] L')\n",
        "_ = ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axes[3 + has_mask_left + has_mask_right]\n",
        "ax.plot(librosa.hz_to_midi(audio_features_right['f0_hz'][:TRIM]))\n",
        "ax.plot(librosa.hz_to_midi(audio_features_mod_right['f0_hz'][:TRIM]))\n",
        "ax.set_ylabel('f0 [midi] R')\n",
        "_ = ax.legend(['Original','Adjusted'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }        
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "nnazgFDd0BW_",
        "outputId": "710d6a1e-d5dd-423e-fa90-547ed8ac7e79"
      },
      "source": [
        "!pip3 install ffmpeg-normalize\n",
        "\n",
        "from scipy.io.wavfile import write as write_audio\n",
        "\n",
        "#@title #Resynthesize Audio\n",
        "\n",
        "af_left = audio_features_left if audio_features_mod_left is None else audio_features_mod_left\n",
        "af_right = audio_features_right if audio_features_mod_right is None else audio_features_mod_right\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "outputs_left = model_left(af_left, training=False)\n",
        "audio_gen_left = model_left.get_audio_from_outputs(outputs_left)\n",
        "outputs_right = model_right(af_right, training=False)\n",
        "audio_gen_right = model_right.get_audio_from_outputs(outputs_right)\n",
        "\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Merge to stereo.\n",
        "audio_gen_left = np.expand_dims(np.squeeze(audio_gen_left.numpy()), axis=1)\n",
        "audio_gen_right = np.expand_dims(np.squeeze(audio_gen_right.numpy()), axis=1)\n",
        "audio_gen_stereo = np.concatenate((audio_gen_left, audio_gen_right), axis=1)\n",
        "\n",
        "# Play\n",
        "print('Resynthesis with primer')\n",
        "play(audio_gen_stereo, sample_rate=sample_rate)\n",
        "\n",
        "WRITE_PATH = OUTPUT_DIR + \"/resynthesis_primer.wav\"\n",
        "\n",
        "write_audio(\"resynthesis_primer.wav\", sample_rate, audio_gen_stereo)\n",
        "write_audio(WRITE_PATH, sample_rate, audio_gen_stereo)\n",
        "\n",
        "!ffmpeg-normalize resynthesis_primer.wav -o resynthesis_primer.wav -t -15 -ar 48000 -f\n",
        "\n",
        "colab_utils.download(\"resynthesis_primer.wav\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id=\"id_6\"> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6a770c53-6f24-45e0-8621-978b3b0abba0\", \"resynthesis_primer.wav\", 1228878)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
